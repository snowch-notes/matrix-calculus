<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.7.32">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Chris Snow">

<title>Matrix Calculus Derivatives: An Introduction</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="matrix-calculus_files/libs/clipboard/clipboard.min.js"></script>
<script src="matrix-calculus_files/libs/quarto-html/quarto.js" type="module"></script>
<script src="matrix-calculus_files/libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="matrix-calculus_files/libs/quarto-html/popper.min.js"></script>
<script src="matrix-calculus_files/libs/quarto-html/tippy.umd.min.js"></script>
<script src="matrix-calculus_files/libs/quarto-html/anchor.min.js"></script>
<link href="matrix-calculus_files/libs/quarto-html/tippy.css" rel="stylesheet">
<link href="matrix-calculus_files/libs/quarto-html/quarto-syntax-highlighting-37eea08aefeeee20ff55810ff984fec1.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="matrix-calculus_files/libs/bootstrap/bootstrap.min.js"></script>
<link href="matrix-calculus_files/libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="matrix-calculus_files/libs/bootstrap/bootstrap-a48ce70e4257b4e66c396f74f0bf4310.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="styles.css">
</head>

<body class="quarto-light">

<div id="quarto-content" class="page-columns page-rows-contents page-layout-article">
<div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
  <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#prerequisites" id="toc-prerequisites" class="nav-link active" data-scroll-target="#prerequisites">Prerequisites</a></li>
  <li><a href="#introduction" id="toc-introduction" class="nav-link" data-scroll-target="#introduction">Introduction</a>
  <ul class="collapse">
  <li><a href="#why-does-this-matter" id="toc-why-does-this-matter" class="nav-link" data-scroll-target="#why-does-this-matter">Why Does This Matter?</a></li>
  <li><a href="#why-matrix-calculus-matters" id="toc-why-matrix-calculus-matters" class="nav-link" data-scroll-target="#why-matrix-calculus-matters">Why Matrix Calculus Matters</a></li>
  </ul></li>
  <li><a href="#mathematical-foundations" id="toc-mathematical-foundations" class="nav-link" data-scroll-target="#mathematical-foundations">Mathematical Foundations</a>
  <ul class="collapse">
  <li><a href="#what-are-scalars-vectors-and-matrices" id="toc-what-are-scalars-vectors-and-matrices" class="nav-link" data-scroll-target="#what-are-scalars-vectors-and-matrices">What Are Scalars, Vectors, and Matrices?</a></li>
  <li><a href="#notation-and-conventions" id="toc-notation-and-conventions" class="nav-link" data-scroll-target="#notation-and-conventions">Notation and Conventions</a></li>
  <li><a href="#single-variable-calculus-review" id="toc-single-variable-calculus-review" class="nav-link" data-scroll-target="#single-variable-calculus-review">Single-Variable Calculus Review</a></li>
  <li><a href="#multiple-variable-calculus" id="toc-multiple-variable-calculus" class="nav-link" data-scroll-target="#multiple-variable-calculus">Multiple-Variable Calculus</a></li>
  </ul></li>
  <li><a href="#the-matrix-calculus-derivatives-table" id="toc-the-matrix-calculus-derivatives-table" class="nav-link" data-scroll-target="#the-matrix-calculus-derivatives-table">The Matrix Calculus Derivatives Table</a></li>
  <li><a href="#detailed-analysis-of-each-case" id="toc-detailed-analysis-of-each-case" class="nav-link" data-scroll-target="#detailed-analysis-of-each-case">Detailed Analysis of Each Case</a>
  <ul class="collapse">
  <li><a href="#case-1-scalar-function-scalar-variable-fracpartial-fpartial-x" id="toc-case-1-scalar-function-scalar-variable-fracpartial-fpartial-x" class="nav-link" data-scroll-target="#case-1-scalar-function-scalar-variable-fracpartial-fpartial-x">Case 1: Scalar Function, Scalar Variable (<span class="math inline">\(\frac{\partial f}{\partial x}\)</span>)</a></li>
  <li><a href="#case-2-scalar-function-vector-variable-fracpartial-fpartial-mathbfx" id="toc-case-2-scalar-function-vector-variable-fracpartial-fpartial-mathbfx" class="nav-link" data-scroll-target="#case-2-scalar-function-vector-variable-fracpartial-fpartial-mathbfx">Case 2: Scalar Function, Vector Variable (<span class="math inline">\(\frac{\partial f}{\partial \mathbf{x}}\)</span>)</a></li>
  <li><a href="#case-3-scalar-function-matrix-variable-fracpartial-fpartial-mathbfx" id="toc-case-3-scalar-function-matrix-variable-fracpartial-fpartial-mathbfx" class="nav-link" data-scroll-target="#case-3-scalar-function-matrix-variable-fracpartial-fpartial-mathbfx">Case 3: Scalar Function, Matrix Variable (<span class="math inline">\(\frac{\partial f}{\partial \mathbf{X}}\)</span>)</a></li>
  <li><a href="#case-4-vector-function-scalar-variable-fracdmathbffdx" id="toc-case-4-vector-function-scalar-variable-fracdmathbffdx" class="nav-link" data-scroll-target="#case-4-vector-function-scalar-variable-fracdmathbffdx">Case 4: Vector Function, Scalar Variable (<span class="math inline">\(\frac{d\mathbf{f}}{dx}\)</span>)</a></li>
  <li><a href="#case-5-vector-function-vector-variable-fracpartial-mathbffpartial-mathbfx" id="toc-case-5-vector-function-vector-variable-fracpartial-mathbffpartial-mathbfx" class="nav-link" data-scroll-target="#case-5-vector-function-vector-variable-fracpartial-mathbffpartial-mathbfx">Case 5: Vector Function, Vector Variable (<span class="math inline">\(\frac{\partial \mathbf{f}}{\partial \mathbf{x}}\)</span>)</a></li>
  <li><a href="#case-6-matrix-function-scalar-variable-fracdmathbffdx" id="toc-case-6-matrix-function-scalar-variable-fracdmathbffdx" class="nav-link" data-scroll-target="#case-6-matrix-function-scalar-variable-fracdmathbffdx">Case 6: Matrix Function, Scalar Variable (<span class="math inline">\(\frac{d\mathbf{F}}{dx}\)</span>)</a></li>
  </ul></li>
  <li><a href="#the-shape-rule-a-universal-principle" id="toc-the-shape-rule-a-universal-principle" class="nav-link" data-scroll-target="#the-shape-rule-a-universal-principle">The Shape Rule: A Universal Principle</a>
  <ul class="collapse">
  <li><a href="#practical-shape-rules" id="toc-practical-shape-rules" class="nav-link" data-scroll-target="#practical-shape-rules">Practical Shape Rules</a></li>
  <li><a href="#memory-aid-for-shape-rules" id="toc-memory-aid-for-shape-rules" class="nav-link" data-scroll-target="#memory-aid-for-shape-rules">Memory Aid for Shape Rules</a></li>
  </ul></li>
  <li><a href="#important-derivative-formulas" id="toc-important-derivative-formulas" class="nav-link" data-scroll-target="#important-derivative-formulas">Important Derivative Formulas</a>
  <ul class="collapse">
  <li><a href="#linear-forms" id="toc-linear-forms" class="nav-link" data-scroll-target="#linear-forms">Linear Forms</a></li>
  <li><a href="#quadratic-forms" id="toc-quadratic-forms" class="nav-link" data-scroll-target="#quadratic-forms">Quadratic Forms</a></li>
  <li><a href="#matrix-trace-derivatives" id="toc-matrix-trace-derivatives" class="nav-link" data-scroll-target="#matrix-trace-derivatives">Matrix Trace Derivatives</a></li>
  <li><a href="#determinant-and-inverse-derivatives" id="toc-determinant-and-inverse-derivatives" class="nav-link" data-scroll-target="#determinant-and-inverse-derivatives">Determinant and Inverse Derivatives</a></li>
  </ul></li>
  <li><a href="#applications-in-machine-learning" id="toc-applications-in-machine-learning" class="nav-link" data-scroll-target="#applications-in-machine-learning">Applications in Machine Learning</a>
  <ul class="collapse">
  <li><a href="#linear-regression" id="toc-linear-regression" class="nav-link" data-scroll-target="#linear-regression">Linear Regression</a></li>
  <li><a href="#principal-component-analysis-pca" id="toc-principal-component-analysis-pca" class="nav-link" data-scroll-target="#principal-component-analysis-pca">Principal Component Analysis (PCA)</a></li>
  <li><a href="#neural-network-backpropagation" id="toc-neural-network-backpropagation" class="nav-link" data-scroll-target="#neural-network-backpropagation">Neural Network Backpropagation</a></li>
  </ul></li>
  <li><a href="#advanced-topics-and-extensions" id="toc-advanced-topics-and-extensions" class="nav-link" data-scroll-target="#advanced-topics-and-extensions">Advanced Topics and Extensions</a>
  <ul class="collapse">
  <li><a href="#higher-order-derivatives-the-hessian-matrix" id="toc-higher-order-derivatives-the-hessian-matrix" class="nav-link" data-scroll-target="#higher-order-derivatives-the-hessian-matrix">Higher-Order Derivatives: The Hessian Matrix</a>
  <ul class="collapse">
  <li><a href="#applications-of-the-hessian" id="toc-applications-of-the-hessian" class="nav-link" data-scroll-target="#applications-of-the-hessian">Applications of the Hessian</a></li>
  </ul></li>
  <li><a href="#matrix-differential-calculus" id="toc-matrix-differential-calculus" class="nav-link" data-scroll-target="#matrix-differential-calculus">Matrix Differential Calculus</a></li>
  <li><a href="#vectorization-and-kronecker-products" id="toc-vectorization-and-kronecker-products" class="nav-link" data-scroll-target="#vectorization-and-kronecker-products">Vectorization and Kronecker Products</a></li>
  </ul></li>
  <li><a href="#computational-considerations" id="toc-computational-considerations" class="nav-link" data-scroll-target="#computational-considerations">Computational Considerations</a>
  <ul class="collapse">
  <li><a href="#automatic-differentiation-autodiff" id="toc-automatic-differentiation-autodiff" class="nav-link" data-scroll-target="#automatic-differentiation-autodiff">Automatic Differentiation (Autodiff)</a>
  <ul class="collapse">
  <li><a href="#forward-mode-autodiff" id="toc-forward-mode-autodiff" class="nav-link" data-scroll-target="#forward-mode-autodiff">Forward Mode Autodiff</a></li>
  <li><a href="#reverse-mode-autodiff-backpropagation" id="toc-reverse-mode-autodiff-backpropagation" class="nav-link" data-scroll-target="#reverse-mode-autodiff-backpropagation">Reverse Mode Autodiff (Backpropagation)</a></li>
  </ul></li>
  <li><a href="#numerical-stability" id="toc-numerical-stability" class="nav-link" data-scroll-target="#numerical-stability">Numerical Stability</a>
  <ul class="collapse">
  <li><a href="#matrix-inversion" id="toc-matrix-inversion" class="nav-link" data-scroll-target="#matrix-inversion">Matrix Inversion</a></li>
  <li><a href="#log-sum-exp-trick" id="toc-log-sum-exp-trick" class="nav-link" data-scroll-target="#log-sum-exp-trick">Log-Sum-Exp Trick</a></li>
  </ul></li>
  <li><a href="#implementation-tips" id="toc-implementation-tips" class="nav-link" data-scroll-target="#implementation-tips">Implementation Tips</a></li>
  </ul></li>
  <li><a href="#common-mistakes-and-pitfalls" id="toc-common-mistakes-and-pitfalls" class="nav-link" data-scroll-target="#common-mistakes-and-pitfalls">Common Mistakes and Pitfalls</a>
  <ul class="collapse">
  <li><a href="#dimension-mismatches" id="toc-dimension-mismatches" class="nav-link" data-scroll-target="#dimension-mismatches">Dimension Mismatches</a></li>
  <li><a href="#chain-rule-errors" id="toc-chain-rule-errors" class="nav-link" data-scroll-target="#chain-rule-errors">Chain Rule Errors</a></li>
  <li><a href="#forgetting-symmetry" id="toc-forgetting-symmetry" class="nav-link" data-scroll-target="#forgetting-symmetry">Forgetting Symmetry</a></li>
  </ul></li>
  <li><a href="#practice-problems" id="toc-practice-problems" class="nav-link" data-scroll-target="#practice-problems">Practice Problems</a>
  <ul class="collapse">
  <li><a href="#basic-problems" id="toc-basic-problems" class="nav-link" data-scroll-target="#basic-problems">Basic Problems</a></li>
  <li><a href="#intermediate-problems" id="toc-intermediate-problems" class="nav-link" data-scroll-target="#intermediate-problems">Intermediate Problems</a></li>
  <li><a href="#advanced-problems" id="toc-advanced-problems" class="nav-link" data-scroll-target="#advanced-problems">Advanced Problems</a></li>
  </ul></li>
  <li><a href="#summary-and-key-takeaways" id="toc-summary-and-key-takeaways" class="nav-link" data-scroll-target="#summary-and-key-takeaways">Summary and Key Takeaways</a>
  <ul class="collapse">
  <li><a href="#the-fundamental-structure" id="toc-the-fundamental-structure" class="nav-link" data-scroll-target="#the-fundamental-structure">The Fundamental Structure</a></li>
  <li><a href="#the-shape-rule-is-your-friend" id="toc-the-shape-rule-is-your-friend" class="nav-link" data-scroll-target="#the-shape-rule-is-your-friend">The Shape Rule is Your Friend</a></li>
  <li><a href="#essential-formulas-to-remember" id="toc-essential-formulas-to-remember" class="nav-link" data-scroll-target="#essential-formulas-to-remember">Essential Formulas to Remember</a></li>
  <li><a href="#the-big-picture" id="toc-the-big-picture" class="nav-link" data-scroll-target="#the-big-picture">The Big Picture</a></li>
  </ul></li>
  <li><a href="#quick-reference-guide" id="toc-quick-reference-guide" class="nav-link" data-scroll-target="#quick-reference-guide">Quick Reference Guide</a>
  <ul class="collapse">
  <li><a href="#common-derivatives" id="toc-common-derivatives" class="nav-link" data-scroll-target="#common-derivatives">Common Derivatives</a></li>
  <li><a href="#shape-rules-quick-reference" id="toc-shape-rules-quick-reference" class="nav-link" data-scroll-target="#shape-rules-quick-reference">Shape Rules Quick Reference</a></li>
  </ul></li>
  </ul>
<div class="quarto-alternate-formats"><h2>Other Formats</h2><ul><li><a href="matrix-calculus.pdf"><i class="bi bi-file-pdf"></i>PDF</a></li><li><a href="matrix-calculus.epub"><i class="bi bi-file"></i>ePub</a></li></ul></div></nav>
</div>
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Matrix Calculus Derivatives: An Introduction</h1>
<p class="subtitle lead">A work-in-progress introductory tutorial on understanding derivatives in matrix calculus with explanations for those with rusty linear algebra, geometry, and calculus knowledge</p>
</div>



<div class="quarto-title-meta">

    <div>
    <div class="quarto-title-meta-heading">Author</div>
    <div class="quarto-title-meta-contents">
             <p>Chris Snow </p>
          </div>
  </div>
    
  
    
  </div>
  


</header>


<section id="prerequisites" class="level1">
<h1>Prerequisites</h1>
<p>Before diving into matrix calculus, you should be comfortable with:</p>
<ul>
<li><strong>Basic calculus</strong>: derivatives, chain rule, partial derivatives</li>
<li><strong>Linear algebra fundamentals</strong>: vectors, matrices, matrix multiplication, transpose</li>
<li><strong>Basic programming</strong>: familiarity with Python/NumPy is helpful but not required</li>
</ul>
<p><strong>Quick refresher resources:</strong></p>
<ul>
<li>Khan Academy: Linear Algebra and Calculus courses</li>
<li>3Blue1Brown: “Essence of Linear Algebra” YouTube series</li>
<li>NumPy documentation for computational examples</li>
</ul>
</section>
<section id="introduction" class="level1">
<h1>Introduction</h1>
<p>Matrix calculus is a powerful mathematical tool that extends ordinary calculus to handle multiple variables at once.</p>
<p>Think of it this way: ordinary calculus deals with functions like <span class="math inline">\(f(x) = x^2\)</span>, where you have one input and one output.</p>
<p>Matrix calculus deals with more complex situations:</p>
<ul>
<li>Functions with multiple inputs (like <span class="math inline">\(f(x, y, z) = x^2 + y^2 + z^2\)</span>)</li>
<li>Functions with multiple outputs (like position in 3D space)</li>
<li>Functions involving entire arrays of numbers (matrices)</li>
</ul>
<section id="why-does-this-matter" class="level2">
<h2 class="anchored" data-anchor-id="why-does-this-matter">Why Does This Matter?</h2>
<p>In the real world, most problems involve many variables simultaneously.</p>
<p><strong>Examples:</strong></p>
<ul>
<li>A neural network might have millions of parameters that all need to be optimized together</li>
<li>The flight path of an airplane depends on altitude, speed, wind direction, and many other factors</li>
<li>Economic models consider prices, supply, demand, and hundreds of other variables</li>
</ul>
<p>Matrix calculus gives us the tools to handle these complex, multi-variable situations efficiently.</p>
</section>
<section id="why-matrix-calculus-matters" class="level2">
<h2 class="anchored" data-anchor-id="why-matrix-calculus-matters">Why Matrix Calculus Matters</h2>
<p>Matrix calculus is essential for:</p>
<p><strong>Machine Learning:</strong></p>
<ul>
<li>Computing gradients for backpropagation in neural networks</li>
<li>This means figuring out how to adjust millions of parameters to reduce prediction errors</li>
</ul>
<p><strong>Optimization:</strong></p>
<ul>
<li>Finding the best solution when you have many variables to consider</li>
<li>Like finding the minimum cost when you can adjust production, shipping, marketing, etc.</li>
</ul>
<p><strong>Statistics:</strong></p>
<ul>
<li>Analyzing data with many variables (like predicting house prices based on size, location, age, etc.)</li>
<li>Deriving formulas for statistical methods like regression</li>
</ul>
<p><strong>Engineering:</strong></p>
<ul>
<li>Designing control systems that manage multiple inputs and outputs</li>
<li>Signal processing for audio, video, and communications</li>
</ul>
<p><strong>Physics:</strong></p>
<ul>
<li>Describing how fields (like electromagnetic fields) behave in space</li>
<li>Quantum mechanics calculations</li>
</ul>
</section>
</section>
<section id="mathematical-foundations" class="level1">
<h1>Mathematical Foundations</h1>
<p>Before we dive into matrix calculus, let’s review the basic building blocks.</p>
<p>Don’t worry if these concepts feel rusty - we’ll build up slowly and explain everything step by step.</p>
<section id="what-are-scalars-vectors-and-matrices" class="level2">
<h2 class="anchored" data-anchor-id="what-are-scalars-vectors-and-matrices">What Are Scalars, Vectors, and Matrices?</h2>
<p><strong>Scalars:</strong> A scalar is just a single number.</p>
<p>Examples: <span class="math inline">\(5\)</span>, <span class="math inline">\(-2.7\)</span>, <span class="math inline">\(\pi\)</span>, <span class="math inline">\(0\)</span></p>
<p>Think of it as a quantity that has magnitude but no direction.</p>
<p>Examples: temperature (<span class="math inline">\(70^\circ\)</span>F), mass (<span class="math inline">\(150\)</span> pounds), price (<span class="math inline">\(\$50\)</span>)</p>
<p><strong>Vectors:</strong> A vector is an ordered list of numbers.</p>
<p>We usually write vectors as columns:</p>
<p><span class="math display">\[\mathbf{x} = \begin{bmatrix} 3 \\ 1 \\ 4 \end{bmatrix}\]</span></p>
<p>Or sometimes as rows: <span class="math inline">\(\mathbf{x} = [3, 1, 4]\)</span></p>
<p><strong>Physical interpretation:</strong> Vectors represent quantities that have both magnitude and direction.</p>
<p>Examples: velocity (<span class="math inline">\(50\)</span> mph northeast), force (<span class="math inline">\(10\)</span> newtons upward)</p>
<p><strong>Mathematical interpretation:</strong> Vectors represent a point in multi-dimensional space.</p>
<p>The vector <span class="math inline">\([3, 1, 4]\)</span> represents a point that’s <span class="math inline">\(3\)</span> units along the x-axis, <span class="math inline">\(1\)</span> unit along the y-axis, and <span class="math inline">\(4\)</span> units along the z-axis.</p>
<p><strong>Matrices:</strong> A matrix is a rectangular array of numbers arranged in rows and columns.</p>
<p>Example of a <span class="math inline">\(3 \times 2\)</span> matrix (3 rows, 2 columns):</p>
<p><span class="math display">\[\mathbf{A} = \begin{bmatrix} 1 &amp; 2 \\ 3 &amp; 4 \\ 5 &amp; 6 \end{bmatrix}\]</span></p>
<p><strong>Physical interpretation:</strong> Matrices can represent transformations (like rotations, scaling, or shearing).</p>
<p>They can also represent systems of equations or relationships between multiple variables.</p>
<p><strong>Mathematical interpretation:</strong> Matrices are a way to organize and manipulate many numbers at once.</p>
<p>They’re especially useful for representing linear relationships between multiple variables.</p>
</section>
<section id="notation-and-conventions" class="level2">
<h2 class="anchored" data-anchor-id="notation-and-conventions">Notation and Conventions</h2>
<p><strong>Important</strong>: We use consistent notation throughout this document to avoid confusion.</p>
<p><strong>Definition - Scalar</strong>: A scalar is a single real number: <span class="math inline">\(x \in \mathbb{R}\)</span></p>
<p>The symbol <span class="math inline">\(\mathbb{R}\)</span> represents the set of all real numbers (positive, negative, and zero).</p>
<p><strong>Definition - Vector</strong>: A vector is an ordered array of scalars: <span class="math inline">\(\mathbf{x} = [x_1, x_2, \ldots, x_n]^T \in \mathbb{R}^n\)</span></p>
<p>The superscript <span class="math inline">\(T\)</span> means “transpose” - it flips the vector from a row to a column or vice versa.</p>
<p><span class="math inline">\(\mathbb{R}^n\)</span> means “n-dimensional real space” - the set of all possible vectors with <span class="math inline">\(n\)</span> real number components.</p>
<p><strong>Definition - Matrix</strong>: A matrix is a rectangular array of scalars: <span class="math inline">\(\mathbf{X} \in \mathbb{R}^{m \times n}\)</span> with elements <span class="math inline">\(X_{ij}\)</span>.</p>
<p><span class="math inline">\(\mathbb{R}^{m \times n}\)</span> means the set of all matrices with <span class="math inline">\(m\)</span> rows and <span class="math inline">\(n\)</span> columns.</p>
<p><span class="math inline">\(X_{ij}\)</span> represents the element in the <span class="math inline">\(i\)</span>-th row and <span class="math inline">\(j\)</span>-th column.</p>
<p><strong>Scalars:</strong> lowercase letters (<span class="math inline">\(x\)</span>, <span class="math inline">\(y\)</span>, <span class="math inline">\(z\)</span>, <span class="math inline">\(a\)</span>, <span class="math inline">\(b\)</span>, <span class="math inline">\(c\)</span>)</p>
<ul>
<li>These represent single numbers</li>
<li>Example: <span class="math inline">\(x = 5\)</span></li>
</ul>
<p><strong>Vectors:</strong> lowercase bold letters (<span class="math inline">\(\mathbf{x}\)</span>, <span class="math inline">\(\mathbf{y}\)</span>, <span class="math inline">\(\mathbf{z}\)</span>)</p>
<ul>
<li>These represent lists of numbers (column vectors by default)</li>
<li>Example: <span class="math inline">\(\mathbf{x} = [1, 2, 3]^T\)</span></li>
</ul>
<p><strong>Matrices:</strong> uppercase bold letters (<span class="math inline">\(\mathbf{A}\)</span>, <span class="math inline">\(\mathbf{X}\)</span>, <span class="math inline">\(\mathbf{Y}\)</span>)</p>
<ul>
<li>These represent rectangular arrays of numbers</li>
<li>Example: <span class="math inline">\(\mathbf{A} = \begin{bmatrix} 1 &amp; 2 \\ 3 &amp; 4 \end{bmatrix}\)</span></li>
</ul>
<p><strong>Functions:</strong></p>
<ul>
<li><span class="math inline">\(f\)</span>, <span class="math inline">\(g\)</span>, <span class="math inline">\(h\)</span> for scalar-valued functions (output a single number)</li>
<li><span class="math inline">\(\mathbf{f}\)</span>, <span class="math inline">\(\mathbf{g}\)</span> for vector-valued functions (output multiple numbers)</li>
<li><span class="math inline">\(\mathbf{F}\)</span>, <span class="math inline">\(\mathbf{G}\)</span> for matrix-valued functions (output a matrix)</li>
</ul>
<p><strong>Derivatives:</strong></p>
<ul>
<li><span class="math inline">\(\partial\)</span> (partial derivative symbol)</li>
<li><span class="math inline">\(\nabla\)</span> (gradient operator)</li>
<li><span class="math inline">\(\mathbf{J}\)</span> (Jacobian matrix)</li>
<li><span class="math inline">\(\mathbf{H}\)</span> (Hessian matrix)</li>
</ul>
</section>
<section id="single-variable-calculus-review" class="level2">
<h2 class="anchored" data-anchor-id="single-variable-calculus-review">Single-Variable Calculus Review</h2>
<p><strong>In single-variable calculus:</strong></p>
<ul>
<li>The derivative <span class="math inline">\(f'(x)\)</span> tells us how much <span class="math inline">\(f(x)\)</span> changes when we make a small change to <span class="math inline">\(x\)</span>.</li>
</ul>
<p><strong>Geometric interpretation:</strong></p>
<ul>
<li><span class="math inline">\(f'(x)\)</span> is the slope of the tangent line to the curve <span class="math inline">\(y = f(x)\)</span>.</li>
</ul>
<p><strong>Physical interpretation:</strong></p>
<ul>
<li>If <span class="math inline">\(f(x)\)</span> represents position at time <span class="math inline">\(x\)</span>, then <span class="math inline">\(f'(x)\)</span> is velocity.</li>
</ul>
<p><strong>Rate interpretation:</strong></p>
<ul>
<li>If <span class="math inline">\(f(x)\)</span> represents profit when you sell <span class="math inline">\(x\)</span> items, then <span class="math inline">\(f'(x)\)</span> tells you how much extra profit you get from selling one more item.</li>
</ul>
</section>
<section id="multiple-variable-calculus" class="level2">
<h2 class="anchored" data-anchor-id="multiple-variable-calculus">Multiple-Variable Calculus</h2>
<p><strong>In multiple variables:</strong> When we have functions of multiple variables, the situation becomes more complex.</p>
<p>Consider <span class="math inline">\(f(x, y) = x^2 + y^2\)</span>. This function takes two inputs and gives one output.</p>
<p><strong>The question becomes:</strong> How does <span class="math inline">\(f\)</span> change when we change <span class="math inline">\(x\)</span>? When we change <span class="math inline">\(y\)</span>? When we change both?</p>
<p><strong>This leads to partial derivatives:</strong></p>
<ul>
<li><span class="math inline">\(\frac{\partial f}{\partial x}\)</span> tells us how <span class="math inline">\(f\)</span> changes when we change <span class="math inline">\(x\)</span> while keeping <span class="math inline">\(y\)</span> fixed</li>
<li><span class="math inline">\(\frac{\partial f}{\partial y}\)</span> tells us how <span class="math inline">\(f\)</span> changes when we change <span class="math inline">\(y\)</span> while keeping <span class="math inline">\(x\)</span> fixed</li>
</ul>
<p><strong>When we arrange these partial derivatives into a vector, we get the gradient:</strong></p>
<p><span class="math display">\[\nabla f = \begin{bmatrix} \frac{\partial f}{\partial x} \\ \frac{\partial f}{\partial y} \end{bmatrix}\]</span></p>
<p><strong>The gradient tells us:</strong></p>
<ul>
<li>The direction of steepest increase of the function</li>
<li>How fast the function increases in that direction</li>
</ul>
<div class="callout callout-style-default callout-note callout-titled" title="Key Insight">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Key Insight
</div>
</div>
<div class="callout-body-container callout-body">
<p>In matrix calculus, the derivative must capture how each component of the output changes with respect to each component of the input. This means we need to keep track of many partial derivatives at once, which is where matrices become essential.</p>
</div>
</div>
</section>
</section>
<section id="the-matrix-calculus-derivatives-table" class="level1">
<h1>The Matrix Calculus Derivatives Table</h1>
<p>The heart of matrix calculus can be summarized in a simple table.</p>
<p>This table shows what type of mathematical object you get when you take derivatives of different combinations of inputs and outputs.</p>
<table class="caption-top table">
<colgroup>
<col style="width: 13%">
<col style="width: 22%">
<col style="width: 32%">
<col style="width: 32%">
</colgroup>
<thead>
<tr class="header">
<th><strong>Function Type</strong></th>
<th><strong>Scalar Variable</strong></th>
<th><strong>Vector Variable</strong></th>
<th><strong>Matrix Variable</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>Scalar Function</strong></td>
<td><span class="math inline">\(\frac{df}{dx}\)</span></td>
<td><span class="math inline">\(\frac{\partial f}{\partial \mathbf{x}}\)</span></td>
<td><span class="math inline">\(\frac{\partial f}{\partial \mathbf{X}}\)</span></td>
</tr>
<tr class="even">
<td><strong>Vector Function</strong></td>
<td><span class="math inline">\(\frac{d\mathbf{f}}{dx}\)</span></td>
<td><span class="math inline">\(\frac{\partial \mathbf{f}}{\partial \mathbf{x}}\)</span></td>
<td>—</td>
</tr>
<tr class="odd">
<td><strong>Matrix Function</strong></td>
<td><span class="math inline">\(\frac{d\mathbf{F}}{dx}\)</span></td>
<td>—</td>
<td>—</td>
</tr>
</tbody>
</table>
<p><strong>How to read this table:</strong></p>
<p><strong>Rows</strong> represent what you’re differentiating (the function).</p>
<ul>
<li>“Scalar Function” means a function that outputs a single number</li>
<li>“Vector Function” means a function that outputs multiple numbers (a vector)</li>
<li>“Matrix Function” means a function that outputs a matrix</li>
</ul>
<p><strong>Columns</strong> represent what you’re differentiating with respect to (the variable).</p>
<ul>
<li>“Scalar Variable” means the input is a single number</li>
<li>“Vector Variable” means the input is a vector</li>
<li>“Matrix Variable” means the input is a matrix</li>
</ul>
<p><strong>Entries</strong> show the notation used for that type of derivative.</p>
<p><strong>Dashes (—)</strong> indicate cases that are either rarely used in practice or require advanced tensor notation.</p>
<p><strong>Let’s understand each entry:</strong></p>
<ul>
<li><span class="math inline">\(\frac{df}{dx}\)</span>: A scalar function of a scalar variable. This is ordinary calculus, one input, one output, one derivative.</li>
<li><span class="math inline">\(\frac{\partial f}{\partial \mathbf{x}}\)</span>: A scalar function of a vector variable gives us a gradient vector.</li>
<li><span class="math inline">\(\frac{\partial f}{\partial \mathbf{X}}\)</span>: A scalar function of a matrix variable gives us a matrix of partial derivatives.</li>
<li><span class="math inline">\(\frac{d\mathbf{f}}{dx}\)</span>: A vector function of a scalar variable - we differentiate each component.</li>
<li><span class="math inline">\(\frac{\partial \mathbf{f}}{\partial \mathbf{x}}\)</span>: A vector function of a vector variable gives us the Jacobian matrix.</li>
<li><span class="math inline">\(\frac{d\mathbf{F}}{dx}\)</span>: A matrix function of a scalar variable - we differentiate each matrix element.</li>
</ul>
<p>Don’t worry if this seems abstract now - we’ll go through each case with detailed examples.</p>
</section>
<section id="detailed-analysis-of-each-case" class="level1">
<h1>Detailed Analysis of Each Case</h1>
<p>Now let’s examine each entry in the table with detailed explanations, examples, and interpretations.</p>
<section id="case-1-scalar-function-scalar-variable-fracpartial-fpartial-x" class="level2">
<h2 class="anchored" data-anchor-id="case-1-scalar-function-scalar-variable-fracpartial-fpartial-x">Case 1: Scalar Function, Scalar Variable (<span class="math inline">\(\frac{\partial f}{\partial x}\)</span>)</h2>
<div class="callout callout-style-default callout-note callout-titled" title="Quick Reference">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Quick Reference
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li><strong>Function Type</strong>: <span class="math inline">\(f: \mathbb{R} \to \mathbb{R}\)</span></li>
<li><strong>Example</strong>: <span class="math inline">\(f(x) = x^2\)</span></li>
<li><strong>Input</strong>: Scalar <span class="math inline">\(x\)</span> (e.g., <span class="math inline">\(x = 2\)</span>)</li>
<li><strong>Output</strong>: Scalar <span class="math inline">\(\frac{\partial f}{\partial x}\)</span> (e.g., <span class="math inline">\(2x = 4\)</span>)</li>
<li><strong>Interpretation</strong>: Rate of change</li>
</ul>
</div>
</div>
<p>This is the familiar case from single-variable calculus that you learned in your first calculus course.</p>
<p><strong>Definition</strong>: Given a scalar function <span class="math inline">\(f: \mathbb{R} \to \mathbb{R}\)</span>, the derivative with respect to scalar <span class="math inline">\(x\)</span> is:</p>
<p><span class="math display">\[\frac{\partial f}{\partial x} = \lim_{h \to 0} \frac{f(x + h) - f(x)}{h}\]</span></p>
<p><strong>What this definition means:</strong> We’re asking: “If I change <span class="math inline">\(x\)</span> by a tiny amount <span class="math inline">\(h\)</span>, how much does <span class="math inline">\(f(x)\)</span> change?”</p>
<p>The ratio <span class="math inline">\(\frac{f(x + h) - f(x)}{h}\)</span> gives us the average rate of change over the interval <span class="math inline">\(h\)</span>.</p>
<p>As <span class="math inline">\(h\)</span> gets smaller and smaller (approaches 0), this ratio approaches the instantaneous rate of change.</p>
<p><strong>Simple Example:</strong> Let <span class="math inline">\(f(x) = x^3 + 2x^2 - 5x + 7\)</span></p>
<p>To find the derivative, we use the power rule:</p>
<ul>
<li>The derivative of <span class="math inline">\(x^3\)</span> is <span class="math inline">\(3x^2\)</span></li>
<li>The derivative of <span class="math inline">\(2x^2\)</span> is <span class="math inline">\(4x\)</span></li>
<li>The derivative of <span class="math inline">\(-5x\)</span> is <span class="math inline">\(-5\)</span></li>
<li>The derivative of <span class="math inline">\(7\)</span> (a constant) is <span class="math inline">\(0\)</span></li>
</ul>
<p>Therefore: <span class="math inline">\(\frac{\partial f}{\partial x} = 3x^2 + 4x - 5\)</span></p>
<p><strong>What this derivative tells us:</strong> At any point <span class="math inline">\(x\)</span>, the derivative gives us the instantaneous rate of change of <span class="math inline">\(f\)</span>.</p>
<p><strong>At <span class="math inline">\(x = 0\)</span>:</strong> <span class="math inline">\(\frac{\partial f}{\partial x} = -5\)</span> (the function is decreasing at a rate of 5 units per unit of <span class="math inline">\(x\)</span>)</p>
<p><strong>At <span class="math inline">\(x = 1\)</span>:</strong> <span class="math inline">\(\frac{\partial f}{\partial x} = 3(1)^2 + 4(1) - 5 = 2\)</span> (the function is increasing at a rate of 2 units per unit of <span class="math inline">\(x\)</span>)</p>
<p><strong>Geometric interpretation:</strong> The derivative represents the slope of the tangent line to the curve <span class="math inline">\(y = f(x)\)</span> at any point <span class="math inline">\(x\)</span>.</p>
<p><strong>Physical interpretation:</strong> If <span class="math inline">\(f(x)\)</span> represents position at time <span class="math inline">\(x\)</span>, then <span class="math inline">\(f'(x)\)</span> is velocity.</p>
<p><strong>Economic interpretation:</strong> If <span class="math inline">\(f(x)\)</span> represents profit when selling <span class="math inline">\(x\)</span> items, then <span class="math inline">\(f'(x)\)</span> is marginal profit (extra profit from selling one more item).</p>
</section>
<section id="case-2-scalar-function-vector-variable-fracpartial-fpartial-mathbfx" class="level2">
<h2 class="anchored" data-anchor-id="case-2-scalar-function-vector-variable-fracpartial-fpartial-mathbfx">Case 2: Scalar Function, Vector Variable (<span class="math inline">\(\frac{\partial f}{\partial \mathbf{x}}\)</span>)</h2>
<div class="callout callout-style-default callout-note callout-titled" title="Quick Reference">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Quick Reference
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li><strong>Function Type</strong>: <span class="math inline">\(f: \mathbb{R}^n \to \mathbb{R}\)</span></li>
<li><strong>Example</strong>: <span class="math inline">\(f(\mathbf{x}) = x_1^2 + x_2^2\)</span></li>
<li><strong>Input</strong>: Vector <span class="math inline">\(\mathbf{x} \in \mathbb{R}^n\)</span> (e.g., <span class="math inline">\(\mathbf{x} = [1, 2]^T\)</span>)</li>
<li><strong>Output</strong>: Gradient vector <span class="math inline">\(\nabla f \in \mathbb{R}^n\)</span></li>
<li><strong>Result</strong>: <span class="math inline">\(f(\mathbf{x}) = 1^2 + 2^2 = 5\)</span></li>
<li><strong>Interpretation</strong>: Direction of steepest increase</li>
</ul>
</div>
</div>
<p>This is where things get more interesting and where matrix calculus really begins.</p>
<p>We have a function that takes multiple inputs (arranged in a vector) and produces a single output.</p>
<p><strong>Definition</strong>: Given a scalar function <span class="math inline">\(f: \mathbb{R}^n \to \mathbb{R}\)</span> and vector <span class="math inline">\(\mathbf{x} = [x_1, x_2, \ldots, x_n]^T\)</span>, the gradient is:</p>
<p><span class="math display">\[\nabla f = \frac{\partial f}{\partial \mathbf{x}} = \begin{bmatrix} \frac{\partial f}{\partial x_1} \\ \frac{\partial f}{\partial x_2} \\ \vdots \\ \frac{\partial f}{\partial x_n} \end{bmatrix} \in \mathbb{R}^n\]</span></p>
<p><strong>What this means:</strong> Instead of one derivative, we now have <span class="math inline">\(n\)</span> derivatives - one for each input variable.</p>
<p>Each partial derivative <span class="math inline">\(\frac{\partial f}{\partial x_i}\)</span> tells us how <span class="math inline">\(f\)</span> changes when we change <span class="math inline">\(x_i\)</span> while holding all other variables constant.</p>
<p>The gradient is the vector that collects all these partial derivatives.</p>
<p><strong>Detailed Example:</strong> Let <span class="math inline">\(f(\mathbf{x}) = x_1^2 + 3x_1x_2 + x_2^2\)</span> where <span class="math inline">\(\mathbf{x} = [x_1, x_2]^T\)</span></p>
<p><strong>Step 1: Find <span class="math inline">\(\frac{\partial f}{\partial x_1}\)</span></strong> Treat <span class="math inline">\(x_2\)</span> as a constant and differentiate with respect to <span class="math inline">\(x_1\)</span>: <span class="math inline">\(\frac{\partial f}{\partial x_1} = \frac{\partial}{\partial x_1}(x_1^2 + 3x_1x_2 + x_2^2) = 2x_1 + 3x_2\)</span></p>
<p><strong>Step 2: Find <span class="math inline">\(\frac{\partial f}{\partial x_2}\)</span></strong> Treat <span class="math inline">\(x_1\)</span> as a constant and differentiate with respect to <span class="math inline">\(x_2\)</span>: <span class="math inline">\(\frac{\partial f}{\partial x_2} = \frac{\partial}{\partial x_2}(x_1^2 + 3x_1x_2 + x_2^2) = 3x_1 + 2x_2\)</span></p>
<p><strong>Step 3: Form the gradient vector</strong> <span class="math inline">\(\frac{\partial f}{\partial \mathbf{x}} = \begin{bmatrix} 2x_1 + 3x_2 \\ 3x_1 + 2x_2 \end{bmatrix}\)</span></p>
<p><strong>Numerical example at a specific point:</strong> At the point <span class="math inline">\(\mathbf{x} = [1, 2]^T\)</span>:</p>
<p><span class="math inline">\(\frac{\partial f}{\partial \mathbf{x}} \bigg|_{\mathbf{x} = [1,2]^T} = \begin{bmatrix} 2(1) + 3(2) \\ 3(1) + 2(2) \end{bmatrix} = \begin{bmatrix} 8 \\ 7 \end{bmatrix}\)</span></p>
<p><strong>What this means:</strong> At the point <span class="math inline">\((1, 2)\)</span>, if we increase <span class="math inline">\(x_1\)</span> by a small amount while keeping <span class="math inline">\(x_2 = 2\)</span>, the function increases at a rate of <span class="math inline">\(8\)</span>.</p>
<p>If we increase <span class="math inline">\(x_2\)</span> by a small amount while keeping <span class="math inline">\(x_1 = 1\)</span>, the function increases at a rate of <span class="math inline">\(7\)</span>.</p>
<p><strong>Geometric interpretation:</strong> The gradient vector points in the direction of steepest increase of the function.</p>
<p><strong>Physical interpretation:</strong> If <span class="math inline">\(f\)</span> represents elevation on a hill and <span class="math inline">\(\mathbf{x}\)</span> represents your position, the gradient points uphill in the steepest direction.</p>
<p><strong>Magnitude interpretation:</strong> The magnitude (length) of the gradient vector tells us how steep the hill is in that direction.</p>
<p><strong>Optimization connection:</strong> In optimization, we often want to find where <span class="math inline">\(\nabla f = \mathbf{0}\)</span> (the zero vector).</p>
<p>These are critical points where the function might have local minima, maxima, or saddle points.</p>
<div class="callout callout-style-default callout-note callout-titled" title="Key Point">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Key Point
</div>
</div>
<div class="callout-body-container callout-body">
<p>The gradient is fundamental to gradient descent optimization.</p>
<p>To minimize <span class="math inline">\(f(\mathbf{x})\)</span>, we update <span class="math inline">\(\mathbf{x}\)</span> in the direction opposite to the gradient: <span class="math display">\[\mathbf{x}_{k+1} = \mathbf{x}_k - \alpha \nabla f(\mathbf{x}_k)\]</span></p>
<p>where <span class="math inline">\(\alpha &gt; 0\)</span> is the learning rate (step size).</p>
<p>This is like rolling a ball downhill - it naturally moves in the direction opposite to the gradient.</p>
</div>
</div>
</section>
<section id="case-3-scalar-function-matrix-variable-fracpartial-fpartial-mathbfx" class="level2">
<h2 class="anchored" data-anchor-id="case-3-scalar-function-matrix-variable-fracpartial-fpartial-mathbfx">Case 3: Scalar Function, Matrix Variable (<span class="math inline">\(\frac{\partial f}{\partial \mathbf{X}}\)</span>)</h2>
<div class="callout callout-style-default callout-note callout-titled" title="Quick Reference">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Quick Reference
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li><strong>Function Type</strong>: <span class="math inline">\(f: \mathbb{R}^{m \times n} \to \mathbb{R}\)</span></li>
<li><strong>Example</strong>: <span class="math inline">\(f(\mathbf{X}) = \text{tr}(\mathbf{X})\)</span></li>
<li><strong>Input</strong>: Matrix <span class="math inline">\(\mathbf{X} \in \mathbb{R}^{m \times n}\)</span> (e.g., <span class="math inline">\(\mathbf{X} = \begin{bmatrix} 1 &amp; 2 \\ 3 &amp; 4 \end{bmatrix}\)</span>)</li>
<li><strong>Output</strong>: Matrix <span class="math inline">\(\frac{\partial f}{\partial \mathbf{X}} \in \mathbb{R}^{m \times n}\)</span></li>
<li><strong>Result</strong>: <span class="math inline">\(f(\mathbf{X}) = \text{tr}(\mathbf{X}) = 1 + 4 = 5\)</span></li>
<li><strong>Interpretation</strong>: Sensitivity to each matrix element</li>
</ul>
</div>
</div>
<p>Now we consider functions that take an entire matrix as input and produce a single scalar output.</p>
<p>This might seem abstract, but it’s actually very common in applications.</p>
<p><strong>Definition</strong>: Given a scalar function <span class="math inline">\(f: \mathbb{R}^{m \times n} \to \mathbb{R}\)</span> and matrix <span class="math inline">\(\mathbf{X} \in \mathbb{R}^{m \times n}\)</span>, the derivative is:</p>
<p><span class="math display">\[\frac{\partial f}{\partial \mathbf{X}} = \left[ \frac{\partial f}{\partial X_{ij}} \right] \in \mathbb{R}^{m \times n}\]</span></p>
<p><strong>What this means:</strong> We compute the partial derivative of <span class="math inline">\(f\)</span> with respect to each element of the matrix <span class="math inline">\(\mathbf{X}\)</span>.</p>
<p>The result is a matrix of the same size as <span class="math inline">\(\mathbf{X}\)</span>, where each element is a partial derivative.</p>
<p><strong>Example 1: The Trace Function</strong> The trace of a square matrix is the sum of its diagonal elements.</p>
<p>For a <span class="math inline">\(3 \times 3\)</span> matrix: <span class="math inline">\(\text{tr}(\mathbf{X}) = X_{11} + X_{22} + X_{33}\)</span></p>
<p>Let <span class="math inline">\(f(\mathbf{X}) = \text{tr}(\mathbf{X})\)</span> where <span class="math inline">\(\mathbf{X} \in \mathbb{R}^{n \times n}\)</span>.</p>
<p><strong>Step 1: Understand what <span class="math inline">\(\text{tr}(\mathbf{X})\)</span> means</strong> <span class="math inline">\(\text{tr}(\mathbf{X}) = X_{11} + X_{22} + \cdots + X_{nn} = \sum_{i=1}^n X_{ii}\)</span></p>
<p><strong>Step 2: Find the partial derivatives</strong> <span class="math inline">\(\frac{\partial f}{\partial X_{ij}} = \frac{\partial}{\partial X_{ij}}(X_{11} + X_{22} + \cdots + X_{nn})\)</span></p>
<p><strong>Case 1: <span class="math inline">\(i = j\)</span> (diagonal elements)</strong> <span class="math inline">\(\frac{\partial f}{\partial X_{ii}} = 1\)</span> (because <span class="math inline">\(X_{ii}\)</span> appears once in the sum)</p>
<p><strong>Case 2: <span class="math inline">\(i \neq j\)</span> (off-diagonal elements)</strong> <span class="math inline">\(\frac{\partial f}{\partial X_{ij}} = 0\)</span> (because <span class="math inline">\(X_{ij}\)</span> doesn’t appear in the sum at all)</p>
<p><strong>Step 3: Form the derivative matrix</strong> <span class="math inline">\(\frac{\partial f}{\partial \mathbf{X}} = \mathbf{I}_n\)</span> (the <span class="math inline">\(n \times n\)</span> identity matrix)</p>
<p><strong>What this means:</strong> The trace function is “sensitive” only to changes in diagonal elements.</p>
<p>If you change any diagonal element by <span class="math inline">\(1\)</span>, the trace increases by <span class="math inline">\(1\)</span>.</p>
<p>If you change any off-diagonal element, the trace doesn’t change at all.</p>
<p><strong>Example 2: The Frobenius Norm Squared</strong> The Frobenius norm of a matrix is like the “length” of the matrix when viewed as a big vector.</p>
<p><span class="math inline">\(f(\mathbf{X}) = \|\mathbf{X}\|_F^2 = \sum_{i=1}^m \sum_{j=1}^n X_{ij}^2\)</span></p>
<p>This is the sum of squares of all elements in the matrix.</p>
<p><strong>Step 1: Find the partial derivative</strong> <span class="math inline">\(\frac{\partial f}{\partial X_{ij}} = \frac{\partial}{\partial X_{ij}}\left(\sum_{k=1}^m \sum_{l=1}^n X_{kl}^2\right) = 2X_{ij}\)</span></p>
<p><strong>Step 2: Form the derivative matrix</strong> <span class="math inline">\(\frac{\partial f}{\partial \mathbf{X}} = 2\mathbf{X}\)</span></p>
<p><strong>What this means:</strong> The rate of change of the Frobenius norm squared is proportional to the matrix itself.</p>
<p>Large elements contribute more to the rate of change than small elements.</p>
<p><strong>Applications:</strong> This result is crucial in matrix optimization problems and regularization techniques.</p>
<p>For example, in machine learning, we often add a term like <span class="math inline">\(\lambda \|\mathbf{W}\|_F^2\)</span> to prevent weights from getting too large.</p>
</section>
<section id="case-4-vector-function-scalar-variable-fracdmathbffdx" class="level2">
<h2 class="anchored" data-anchor-id="case-4-vector-function-scalar-variable-fracdmathbffdx">Case 4: Vector Function, Scalar Variable (<span class="math inline">\(\frac{d\mathbf{f}}{dx}\)</span>)</h2>
<div class="callout callout-style-default callout-note callout-titled" title="Quick Reference">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Quick Reference
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li><strong>Function Type</strong>: <span class="math inline">\(\mathbf{f}: \mathbb{R} \to \mathbb{R}^m\)</span></li>
<li><strong>Example</strong>: <span class="math inline">\(\mathbf{f}(x) = [x^2, x^3]^T\)</span></li>
<li><strong>Input</strong>: Scalar <span class="math inline">\(x\)</span> (e.g., <span class="math inline">\(x = 2\)</span>)</li>
<li><strong>Output</strong>: Vector <span class="math inline">\(\frac{d\mathbf{f}}{dx} \in \mathbb{R}^m\)</span></li>
<li><strong>Result</strong>: <span class="math inline">\(\frac{d\mathbf{f}}{dx} = [2x, 3x^2]^T = [4, 12]^T\)</span></li>
<li><strong>Interpretation</strong>: Rate of change for each component</li>
</ul>
</div>
</div>
<p>Now we consider functions that take a single scalar input and produce multiple scalar outputs (arranged in a vector).</p>
<p><strong>Definition</strong>: Given a vector function <span class="math inline">\(\mathbf{f}: \mathbb{R} \to \mathbb{R}^m\)</span> with <span class="math inline">\(\mathbf{f}(x) = [f_1(x), f_2(x), \ldots, f_m(x)]^T\)</span>, the derivative is:</p>
<p><span class="math display">\[\frac{d\mathbf{f}}{dx} = \begin{bmatrix} \frac{df_1}{dx} \\ \frac{df_2}{dx} \\ \vdots \\ \frac{df_m}{dx} \end{bmatrix} \in \mathbb{R}^m\]</span></p>
<p><strong>What this means:</strong> We have <span class="math inline">\(m\)</span> different functions, each depending on the same scalar variable <span class="math inline">\(x\)</span>.</p>
<p>We differentiate each function separately with respect to <span class="math inline">\(x\)</span>.</p>
<p>The result is a vector where each component is the derivative of the corresponding component function.</p>
<p><strong>Detailed Example:</strong> Let <span class="math inline">\(\mathbf{f}(t) = [\cos(t), \sin(t), t^2]^T\)</span> where <span class="math inline">\(t\)</span> is a scalar parameter.</p>
<p><strong>Step 1: Identify the component functions</strong></p>
<ul>
<li><span class="math inline">\(f_1(t) = \cos(t)\)</span></li>
<li><span class="math inline">\(f_2(t) = \sin(t)\)</span></li>
<li><span class="math inline">\(f_3(t) = t^2\)</span></li>
</ul>
<p><strong>Step 2: Differentiate each component</strong></p>
<ul>
<li><span class="math inline">\(\frac{df_1}{dt} = \frac{d}{dt}[\cos(t)] = -\sin(t)\)</span></li>
<li><span class="math inline">\(\frac{df_2}{dt} = \frac{d}{dt}[\sin(t)] = \cos(t)\)</span></li>
<li><span class="math inline">\(\frac{df_3}{dt} = \frac{d}{dt}[t^2] = 2t\)</span></li>
</ul>
<p><strong>Step 3: Form the derivative vector</strong> <span class="math inline">\(\frac{d\mathbf{f}}{dt} = \begin{bmatrix} -\sin(t) \\ \cos(t) \\ 2t \end{bmatrix}\)</span></p>
<p><strong>Physical interpretation:</strong> If <span class="math inline">\(\mathbf{f}(t)\)</span> represents the position vector of a particle moving in 3D space as a function of time <span class="math inline">\(t\)</span>, then <span class="math inline">\(\frac{d\mathbf{f}}{dt}\)</span> is the velocity vector.</p>
<p><strong>Geometric interpretation:</strong> If <span class="math inline">\(\mathbf{f}(t)\)</span> traces out a curve in 3D space, then <span class="math inline">\(\frac{d\mathbf{f}}{dt}\)</span> is the tangent vector to that curve.</p>
<p><strong>Component analysis:</strong> At any time <span class="math inline">\(t\)</span>:</p>
<ul>
<li>The x-component of velocity is <span class="math inline">\(-\sin(t)\)</span></li>
<li>The y-component of velocity is <span class="math inline">\(\cos(t)\)</span></li>
<li>The z-component of velocity is <span class="math inline">\(2t\)</span></li>
</ul>
<p><strong>Specific example at <span class="math inline">\(t = \frac{\pi}{2}\)</span>:</strong> <span class="math inline">\(\frac{d\mathbf{f}}{dt} \bigg|_{t = \frac{\pi}{2}} = \begin{bmatrix} -\sin\left(\frac{\pi}{2}\right) \\ \cos\left(\frac{\pi}{2}\right) \\ 2\left(\frac{\pi}{2}\right) \end{bmatrix} = \begin{bmatrix} -1 \\ 0 \\ \pi \end{bmatrix}\)</span></p>
<p><strong>What this means:</strong> At time <span class="math inline">\(t = \frac{\pi}{2}\)</span>, the particle is moving in the negative x-direction at speed <span class="math inline">\(1\)</span>, not moving in the y-direction, and moving in the positive z-direction at speed <span class="math inline">\(\pi\)</span>.</p>
</section>
<section id="case-5-vector-function-vector-variable-fracpartial-mathbffpartial-mathbfx" class="level2">
<h2 class="anchored" data-anchor-id="case-5-vector-function-vector-variable-fracpartial-mathbffpartial-mathbfx">Case 5: Vector Function, Vector Variable (<span class="math inline">\(\frac{\partial \mathbf{f}}{\partial \mathbf{x}}\)</span>)</h2>
<div class="callout callout-style-default callout-note callout-titled" title="Quick Reference">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Quick Reference
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li><strong>Function Type</strong>: <span class="math inline">\(\mathbf{f}: \mathbb{R}^n \to \mathbb{R}^m\)</span></li>
<li><strong>Example</strong>: <span class="math inline">\(\mathbf{f}(\mathbf{x}) = [x_1^2 + x_2, x_1x_2]^T\)</span></li>
<li><strong>Input</strong>: Vector <span class="math inline">\(\mathbf{x} \in \mathbb{R}^n\)</span> (e.g., <span class="math inline">\(\mathbf{x} = [1, 2]^T\)</span>)</li>
<li><strong>Output</strong>: Jacobian matrix <span class="math inline">\(\mathbf{J} \in \mathbb{R}^{m \times n}\)</span></li>
<li><strong>Result</strong>: <span class="math inline">\(\mathbf{J} = \begin{bmatrix} 2x_1 &amp; 1 \\ x_2 &amp; x_1 \end{bmatrix} = \begin{bmatrix} 2 &amp; 1 \\ 2 &amp; 1 \end{bmatrix}\)</span></li>
<li><strong>Interpretation</strong>: Linear approximation of function</li>
</ul>
</div>
</div>
<p>This is one of the most important cases in matrix calculus.</p>
<p>We have a function that takes multiple inputs and produces multiple outputs.</p>
<p>This produces the Jacobian matrix, which is fundamental to multivariable calculus and optimization.</p>
<p><strong>Definition</strong>: Given a vector function <span class="math inline">\(\mathbf{f}: \mathbb{R}^n \to \mathbb{R}^m\)</span> with <span class="math inline">\(\mathbf{f}(\mathbf{x}) = [f_1(\mathbf{x}), f_2(\mathbf{x}), \ldots, f_m(\mathbf{x})]^T\)</span>, the Jacobian matrix is:</p>
<p><span class="math display">\[
\mathbf{J} = \frac{\partial \mathbf{f}}{\partial \mathbf{x}} = \begin{bmatrix}
\frac{\partial f_1}{\partial x_1} &amp; \frac{\partial f_1}{\partial x_2} &amp; \cdots &amp; \frac{\partial f_1}{\partial x_n} \\
\frac{\partial f_2}{\partial x_1} &amp; \frac{\partial f_2}{\partial x_2} &amp; \cdots &amp; \frac{\partial f_2}{\partial x_n} \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
\frac{\partial f_m}{\partial x_1} &amp; \frac{\partial f_m}{\partial x_2} &amp; \cdots &amp; \frac{\partial f_m}{\partial x_n}
\end{bmatrix} \in \mathbb{R}^{m \times n}
\]</span></p>
<p><strong>What this means:</strong> We have <span class="math inline">\(m\)</span> functions, each depending on <span class="math inline">\(n\)</span> variables.</p>
<p>The Jacobian is an <span class="math inline">\(m \times n\)</span> matrix where:</p>
<ul>
<li>Each row contains the gradient of one component function</li>
<li>Each column shows how all outputs change with respect to one input</li>
</ul>
<p><strong>The <span class="math inline">\((i,j)\)</span> element of the Jacobian is <span class="math inline">\(\frac{\partial f_i}{\partial x_j}\)</span>:</strong></p>
<ul>
<li>This tells us how the <span class="math inline">\(i\)</span>-th output changes when we change the <span class="math inline">\(j\)</span>-th input</li>
</ul>
<p><strong>Detailed Example:</strong> Let <span class="math inline">\(\mathbf{f}(\mathbf{x}) = [x_1^2 + x_2, x_1x_2, \sin(x_1) + \cos(x_2)]^T\)</span> where <span class="math inline">\(\mathbf{x} = [x_1, x_2]^T\)</span></p>
<p><strong>Step 1: Identify the component functions</strong></p>
<ul>
<li><span class="math inline">\(f_1(\mathbf{x}) = x_1^2 + x_2\)</span></li>
<li><span class="math inline">\(f_2(\mathbf{x}) = x_1x_2\)</span></li>
<li><span class="math inline">\(f_3(\mathbf{x}) = \sin(x_1) + \cos(x_2)\)</span></li>
</ul>
<p><strong>Step 2: Compute partial derivatives for each row</strong></p>
<p><strong>Row 1 (gradient of <span class="math inline">\(f_1\)</span>):</strong></p>
<ul>
<li><span class="math inline">\(\frac{\partial f_1}{\partial x_1} = \frac{\partial}{\partial x_1}(x_1^2 + x_2) = 2x_1\)</span></li>
<li><span class="math inline">\(\frac{\partial f_1}{\partial x_2} = \frac{\partial}{\partial x_2}(x_1^2 + x_2) = 1\)</span></li>
</ul>
<p><strong>Row 2 (gradient of <span class="math inline">\(f_2\)</span>):</strong></p>
<ul>
<li><span class="math inline">\(\frac{\partial f_2}{\partial x_1} = \frac{\partial}{\partial x_1}(x_1x_2) = x_2\)</span></li>
<li><span class="math inline">\(\frac{\partial f_2}{\partial x_2} = \frac{\partial}{\partial x_2}(x_1x_2) = x_1\)</span></li>
</ul>
<p><strong>Row 3 (gradient of <span class="math inline">\(f_3\)</span>):</strong></p>
<ul>
<li><span class="math inline">\(\frac{\partial f_3}{\partial x_1} = \frac{\partial}{\partial x_1}(\sin(x_1) + \cos(x_2)) = \cos(x_1)\)</span></li>
<li><span class="math inline">\(\frac{\partial f_3}{\partial x_2} = \frac{\partial}{\partial x_2}(\sin(x_1) + \cos(x_2)) = -\sin(x_2)\)</span></li>
</ul>
<p><strong>Step 3: Form the Jacobian matrix</strong></p>
<p><span class="math display">\[
\mathbf{J} = \begin{bmatrix}
2x_1 &amp; 1 \\
x_2 &amp; x_1 \\
\cos(x_1) &amp; -\sin(x_2)
\end{bmatrix}
\]</span></p>
<p><strong>Numerical example at a specific point:</strong> At the point <span class="math inline">\(\mathbf{x} = [1, 0]^T\)</span>:</p>
<p><span class="math display">\[
\mathbf{J} \bigg|_{\mathbf{x} = [1,0]^T} = \begin{bmatrix}
2(1) &amp; 1 \\
0 &amp; 1 \\
\cos(1) &amp; -\sin(0)
\end{bmatrix} = \begin{bmatrix}
2 &amp; 1 \\
0 &amp; 1 \\
\cos(1) &amp; 0
\end{bmatrix}
\]</span></p>
<p><strong>What each element means:</strong></p>
<ul>
<li><span class="math inline">\(J_{11} = 2\)</span>: If we increase <span class="math inline">\(x_1\)</span> slightly, <span class="math inline">\(f_1\)</span> increases at rate <span class="math inline">\(2\)</span></li>
<li><span class="math inline">\(J_{12} = 1\)</span>: If we increase <span class="math inline">\(x_2\)</span> slightly, <span class="math inline">\(f_1\)</span> increases at rate <span class="math inline">\(1\)</span></li>
<li><span class="math inline">\(J_{21} = 0\)</span>: If we increase <span class="math inline">\(x_1\)</span> slightly, <span class="math inline">\(f_2\)</span> doesn’t change (at this point)</li>
<li><span class="math inline">\(J_{22} = 1\)</span>: If we increase <span class="math inline">\(x_2\)</span> slightly, <span class="math inline">\(f_2\)</span> increases at rate <span class="math inline">\(1\)</span></li>
<li>And so on…</li>
</ul>
<div class="callout callout-style-default callout-note callout-titled" title="Theorem (Chain Rule for Jacobians)">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Theorem (Chain Rule for Jacobians)
</div>
</div>
<div class="callout-body-container callout-body">
<p>If <span class="math inline">\(\mathbf{g}: \mathbb{R}^p \to \mathbb{R}^n\)</span> and <span class="math inline">\(\mathbf{f}: \mathbb{R}^n \to \mathbb{R}^m\)</span>, then the Jacobian of the composition <span class="math inline">\(\mathbf{h}(\mathbf{x}) = \mathbf{f}(\mathbf{g}(\mathbf{x}))\)</span> is:</p>
<p><span class="math display">\[\frac{\partial \mathbf{h}}{\partial \mathbf{x}} = \left( \frac{\partial \mathbf{f}}{\partial \mathbf{g}} \right) \left( \frac{\partial \mathbf{g}}{\partial \mathbf{x}} \right)\]</span></p>
<p><strong>What the chain rule means:</strong> If you have a composition of functions (one function feeding into another), the derivative of the composition is the product of the individual Jacobians.</p>
<p><strong>This is the foundation of backpropagation in neural networks:</strong> Neural networks are compositions of many simple functions, and we use the chain rule to compute how the final output depends on all the parameters.</p>
</div>
</div>
<p><strong>Applications of the Jacobian:</strong></p>
<p><strong>Newton’s Method for solving equations:</strong> To solve <span class="math inline">\(\mathbf{f}(\mathbf{x}) = \mathbf{0}\)</span>, we use the update rule: <span class="math inline">\(\mathbf{x}_{k+1} = \mathbf{x}_k - \mathbf{J}(\mathbf{x}_k)^{-1} \mathbf{f}(\mathbf{x}_k)\)</span></p>
<p><strong>Linear approximation:</strong> Near a point <span class="math inline">\(\mathbf{a}\)</span>, we can approximate: <span class="math inline">\(\mathbf{f}(\mathbf{x}) \approx \mathbf{f}(\mathbf{a}) + \mathbf{J}(\mathbf{a})(\mathbf{x} - \mathbf{a})\)</span></p>
<p><strong>Change of variables in integration:</strong> When changing variables in multiple integrals, the determinant <span class="math inline">\(|\det(\mathbf{J})|\)</span> appears as the “scaling factor.”</p>
</section>
<section id="case-6-matrix-function-scalar-variable-fracdmathbffdx" class="level2">
<h2 class="anchored" data-anchor-id="case-6-matrix-function-scalar-variable-fracdmathbffdx">Case 6: Matrix Function, Scalar Variable (<span class="math inline">\(\frac{d\mathbf{F}}{dx}\)</span>)</h2>
<div class="callout callout-style-default callout-note callout-titled" title="Quick Reference">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Quick Reference
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li><strong>Function Type</strong>: <span class="math inline">\(\mathbf{F}: \mathbb{R} \to \mathbb{R}^{m \times n}\)</span></li>
<li><strong>Example</strong>: <span class="math inline">\(\mathbf{F}(x) = \begin{bmatrix} x &amp; x^2 \\ x^3 &amp; x^4 \end{bmatrix}\)</span></li>
<li><strong>Input</strong>: Scalar <span class="math inline">\(x\)</span> (e.g., <span class="math inline">\(x = 2\)</span>)</li>
<li><strong>Output</strong>: Matrix <span class="math inline">\(\frac{d\mathbf{F}}{dx} \in \mathbb{R}^{m \times n}\)</span></li>
<li><strong>Result</strong>: <span class="math inline">\(\frac{d\mathbf{F}}{dx} = \begin{bmatrix} 1 &amp; 2x \\ 3x^2 &amp; 4x^3 \end{bmatrix} = \begin{bmatrix} 1 &amp; 4 \\ 12 &amp; 32 \end{bmatrix}\)</span></li>
<li><strong>Interpretation</strong>: Element-wise rate of change</li>
</ul>
</div>
</div>
<p>Finally, we consider functions that take a scalar input and produce a matrix output.</p>
<p><strong>Definition</strong>: Given a matrix function <span class="math inline">\(\mathbf{F}: \mathbb{R} \to \mathbb{R}^{m \times n}\)</span> with elements <span class="math inline">\(F_{ij}(x)\)</span>, the derivative is:</p>
<p><span class="math display">\[\frac{d\mathbf{F}}{dx} = \left[ \frac{d F_{ij}}{dx} \right] \in \mathbb{R}^{m \times n}\]</span></p>
<p><strong>What this means:</strong> Each element of the matrix <span class="math inline">\(\mathbf{F}\)</span> is a function of the scalar <span class="math inline">\(x\)</span>.</p>
<p>We differentiate each element separately.</p>
<p>The result is a matrix of the same size, where each element is the derivative of the corresponding element in <span class="math inline">\(\mathbf{F}\)</span>.</p>
<p><strong>Detailed Example:</strong> Let <span class="math inline">\(\mathbf{F}(t) = \begin{bmatrix} \cos(t) &amp; \sin(t) \\ -\sin(t) &amp; \cos(t) \end{bmatrix}\)</span> (a <span class="math inline">\(2 \times 2\)</span> rotation matrix)</p>
<p><strong>Step 1: Identify each matrix element as a function of <span class="math inline">\(t\)</span></strong></p>
<ul>
<li><span class="math inline">\(F_{11}(t) = \cos(t)\)</span></li>
<li><span class="math inline">\(F_{12}(t) = \sin(t)\)</span></li>
<li><span class="math inline">\(F_{21}(t) = -\sin(t)\)</span></li>
<li><span class="math inline">\(F_{22}(t) = \cos(t)\)</span></li>
</ul>
<p><strong>Step 2: Differentiate each element</strong></p>
<ul>
<li><span class="math inline">\(\frac{d F_{11}}{dt} = \frac{d}{dt}[\cos(t)] = -\sin(t)\)</span></li>
<li><span class="math inline">\(\frac{d F_{12}}{dt} = \frac{d}{dt}[\sin(t)] = \cos(t)\)</span></li>
<li><span class="math inline">\(\frac{d F_{21}}{dt} = \frac{d}{dt}[-\sin(t)] = -\cos(t)\)</span></li>
<li><span class="math inline">\(\frac{d F_{22}}{dt} = \frac{d}{dt}[\cos(t)] = -\sin(t)\)</span></li>
</ul>
<p><strong>Step 3: Form the derivative matrix</strong> <span class="math inline">\(\frac{d\mathbf{F}}{dt} = \begin{bmatrix} -\sin(t) &amp; \cos(t) \\ -\cos(t) &amp; -\sin(t) \end{bmatrix}\)</span></p>
<p><strong>Physical interpretation:</strong> The original matrix <span class="math inline">\(\mathbf{F}(t)\)</span> represents a rotation by angle <span class="math inline">\(t\)</span>.</p>
<p>The derivative <span class="math inline">\(\frac{d\mathbf{F}}{dt}\)</span> represents the rate of rotation.</p>
<p><strong>Geometric interpretation:</strong> As <span class="math inline">\(t\)</span> changes, the matrix <span class="math inline">\(\mathbf{F}(t)\)</span> rotates vectors in the plane.</p>
<p>The derivative tells us how fast this rotation is happening.</p>
</section>
</section>
<section id="the-shape-rule-a-universal-principle" class="level1">
<h1>The Shape Rule: A Universal Principle</h1>
<p>One of the most important concepts in matrix calculus is understanding the dimensions (shape) of derivatives.</p>
<p>This helps you check your work and understand the structure of the mathematics.</p>
<div class="callout callout-style-default callout-note callout-titled" title="Theorem (The Shape Rule)">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Theorem (The Shape Rule)
</div>
</div>
<div class="callout-body-container callout-body">
<p>The derivative <span class="math inline">\(\frac{\partial \mathbf{Y}}{\partial \mathbf{X}}\)</span> has dimensions that are determined by the dimensions of <span class="math inline">\(\mathbf{Y}\)</span> and <span class="math inline">\(\mathbf{X}\)</span>.</p>
<p>If <span class="math inline">\(\mathbf{Y}\)</span> has <span class="math inline">\(p \times q\)</span> elements and <span class="math inline">\(\mathbf{X}\)</span> has <span class="math inline">\(r \times s\)</span> elements, then the derivative conceptually lives in a space with <span class="math inline">\(p \times q \times r \times s\)</span> dimensions.</p>
<p>In practice, we organize these derivatives in a way that makes computational sense.</p>
</div>
</div>
<p><strong>Don’t worry if this seems abstract - let’s look at practical rules:</strong></p>
<section id="practical-shape-rules" class="level2">
<h2 class="anchored" data-anchor-id="practical-shape-rules">Practical Shape Rules</h2>
<p><strong>Rule 1: Scalar function of vector variable</strong> If <span class="math inline">\(f: \mathbb{R}^n \to \mathbb{R}\)</span>, then <span class="math inline">\(\frac{\partial f}{\partial \mathbf{x}} \in \mathbb{R}^n\)</span></p>
<p><strong>Why this makes sense:</strong></p>
<ul>
<li>We have 1 output (scalar)</li>
<li>We have <span class="math inline">\(n\)</span> inputs (vector components)</li>
<li>So we need <span class="math inline">\(n\)</span> partial derivatives (one for each input)</li>
<li>Result: a vector with <span class="math inline">\(n\)</span> components</li>
</ul>
<p><strong>Rule 2: Vector function of vector variable</strong> If <span class="math inline">\(\mathbf{f}: \mathbb{R}^n \to \mathbb{R}^m\)</span>, then <span class="math inline">\(\frac{\partial \mathbf{f}}{\partial \mathbf{x}} \in \mathbb{R}^{m \times n}\)</span></p>
<p><strong>Why this makes sense:</strong></p>
<ul>
<li>We have <span class="math inline">\(m\)</span> outputs (vector components)</li>
<li>We have <span class="math inline">\(n\)</span> inputs (vector components)</li>
<li>For each output, we need the partial derivative with respect to each input</li>
<li>So we need <span class="math inline">\(m \times n\)</span> partial derivatives</li>
<li>Result: an <span class="math inline">\(m \times n\)</span> matrix (<span class="math inline">\(m\)</span> rows, <span class="math inline">\(n\)</span> columns)</li>
</ul>
<p><strong>Rule 3: Scalar function of matrix variable</strong> If <span class="math inline">\(f: \mathbb{R}^{m \times n} \to \mathbb{R}\)</span>, then <span class="math inline">\(\frac{\partial f}{\partial \mathbf{X}} \in \mathbb{R}^{m \times n}\)</span></p>
<p><strong>Why this makes sense:</strong></p>
<ul>
<li>We have 1 output (scalar)</li>
<li>We have <span class="math inline">\(m \times n\)</span> inputs (matrix elements)</li>
<li>We need one partial derivative for each matrix element</li>
<li>Result: a matrix with the same shape as the input matrix</li>
</ul>
</section>
<section id="memory-aid-for-shape-rules" class="level2">
<h2 class="anchored" data-anchor-id="memory-aid-for-shape-rules">Memory Aid for Shape Rules</h2>
<p><strong>The key insight:</strong> The derivative has the same “shape structure” as the variable you’re differentiating with respect to, combined with the output structure.</p>
<p><strong>Simple way to remember:</strong></p>
<ol type="1">
<li>Look at what you’re differentiating (the function output)</li>
<li>Look at what you’re differentiating with respect to (the variable)</li>
<li>The derivative combines information about both</li>
</ol>
<p><strong>Examples to cement the concept:</strong></p>
<p><strong>Gradient example:</strong></p>
<ul>
<li>Function: <span class="math inline">\(f(\mathbf{x})\)</span> where <span class="math inline">\(\mathbf{x} \in \mathbb{R}^3\)</span> (3-dimensional vector)</li>
<li>Output: scalar (1 number)</li>
<li>Derivative: <span class="math inline">\(\nabla f \in \mathbb{R}^3\)</span> (3 numbers - one partial derivative for each input component)</li>
</ul>
<p><strong>Jacobian example:</strong></p>
<ul>
<li>Function: <span class="math inline">\(\mathbf{f}(\mathbf{x})\)</span> where <span class="math inline">\(\mathbf{f} \in \mathbb{R}^2\)</span> and <span class="math inline">\(\mathbf{x} \in \mathbb{R}^3\)</span></li>
<li>Output: 2 numbers (vector with 2 components)</li>
<li>Input: 3 numbers (vector with 3 components)</li>
<li>Derivative: <span class="math inline">\(\mathbf{J} \in \mathbb{R}^{2 \times 3}\)</span> (2x3 matrix - for each of the 2 outputs, we need partial derivatives with respect to each of the 3 inputs)</li>
</ul>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p><strong>Key Point</strong>: The shape rule helps you quickly verify if your derivative calculations are correct.</p>
<p>If the dimensions don’t match what the shape rule predicts, you’ve made an error somewhere.</p>
<p>Always check dimensions as a sanity check!</p>
</div>
</div>
</section>
</section>
<section id="important-derivative-formulas" class="level1">
<h1>Important Derivative Formulas</h1>
<p>Here are the most commonly used derivative formulas in matrix calculus.</p>
<p>Don’t try to memorize these all at once - focus on understanding the patterns and refer back to this section as needed.</p>
<section id="linear-forms" class="level2">
<h2 class="anchored" data-anchor-id="linear-forms">Linear Forms</h2>
<p>Linear forms are expressions where variables appear to the first power only (no squares, products, etc.).</p>
<div class="callout callout-style-default callout-note callout-titled" title="Theorem (Linear Form Derivatives)">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Theorem (Linear Form Derivatives)
</div>
</div>
<div class="callout-body-container callout-body">
<p>Let <span class="math inline">\(\mathbf{a}\)</span> be a constant vector and <span class="math inline">\(\mathbf{A}\)</span> be a constant matrix. Then:</p>
<p><strong>Formula 1:</strong> <span class="math inline">\(\frac{\partial}{\partial \mathbf{x}}(\mathbf{a}^T \mathbf{x}) = \mathbf{a}\)</span></p>
<p><strong>Formula 2:</strong> <span class="math inline">\(\frac{\partial}{\partial \mathbf{x}}(\mathbf{x}^T \mathbf{a}) = \mathbf{a}\)</span></p>
<p><strong>Formula 3:</strong> <span class="math inline">\(\frac{\partial}{\partial \mathbf{x}}(\mathbf{A} \mathbf{x}) = \mathbf{A}^T\)</span></p>
<p><strong>Formula 4:</strong> <span class="math inline">\(\frac{\partial}{\partial \mathbf{x}}(\mathbf{x}^T \mathbf{A}) = \mathbf{A}\)</span></p>
</div>
</div>
<p><strong>Example for Formula 1:</strong> Let <span class="math inline">\(\mathbf{a} = \begin{bmatrix} 2 \\ 3 \\ 1 \end{bmatrix}\)</span> and <span class="math inline">\(\mathbf{x} = \begin{bmatrix} x_1 \\ x_2 \\ x_3 \end{bmatrix}\)</span></p>
<p>Then <span class="math inline">\(\mathbf{a}^T \mathbf{x} = 2x_1 + 3x_2 + x_3\)</span></p>
<p>Taking partial derivatives:</p>
<ul>
<li><span class="math inline">\(\frac{\partial}{\partial x_1}(2x_1 + 3x_2 + x_3) = 2\)</span></li>
<li><span class="math inline">\(\frac{\partial}{\partial x_2}(2x_1 + 3x_2 + x_3) = 3\)</span></li>
<li><span class="math inline">\(\frac{\partial}{\partial x_3}(2x_1 + 3x_2 + x_3) = 1\)</span></li>
</ul>
<p>So <span class="math inline">\(\frac{\partial}{\partial \mathbf{x}}(\mathbf{a}^T \mathbf{x}) = \begin{bmatrix} 2 \\ 3 \\ 1 \end{bmatrix} = \mathbf{a}\)</span></p>
</section>
<section id="quadratic-forms" class="level2">
<h2 class="anchored" data-anchor-id="quadratic-forms">Quadratic Forms</h2>
<p>Quadratic forms involve variables raised to the second power or products of variables.</p>
<div class="callout callout-style-default callout-note callout-titled" title="Theorem (Quadratic Form Derivatives)">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Theorem (Quadratic Form Derivatives)
</div>
</div>
<div class="callout-body-container callout-body">
<p>Let <span class="math inline">\(\mathbf{A}\)</span> be a constant matrix. Then:</p>
<p><strong>Formula 1:</strong> <span class="math inline">\(\frac{\partial}{\partial \mathbf{x}}(\mathbf{x}^T \mathbf{A} \mathbf{x}) = (\mathbf{A} + \mathbf{A}^T) \mathbf{x}\)</span></p>
<p><strong>Formula 2:</strong> <span class="math inline">\(\frac{\partial}{\partial \mathbf{x}}(\mathbf{x}^T \mathbf{x}) = 2\mathbf{x}\)</span></p>
</div>
</div>
<p><strong>Detailed Example for Formula 1:</strong> Let <span class="math inline">\(\mathbf{A} = \begin{bmatrix} 2 &amp; 1 \\ 3 &amp; 4 \end{bmatrix}\)</span> and <span class="math inline">\(\mathbf{x} = \begin{bmatrix} x_1 \\ x_2 \end{bmatrix}\)</span></p>
<p><strong>Step 1: Expand the quadratic form</strong> <span class="math display">\[
\begin{aligned}
\mathbf{x}^T \mathbf{A} \mathbf{x} &amp;= \begin{bmatrix} x_1 &amp; x_2 \end{bmatrix} \begin{bmatrix} 2 &amp; 1 \\ 3 &amp; 4 \end{bmatrix} \begin{bmatrix} x_1 \\ x_2 \end{bmatrix} \\
&amp;= \begin{bmatrix} x_1 &amp; x_2 \end{bmatrix} \begin{bmatrix} 2x_1 + x_2 \\ 3x_1 + 4x_2 \end{bmatrix} \\
&amp;= 2x_1^2 + x_1x_2 + 3x_1x_2 + 4x_2^2 \\
&amp;= 2x_1^2 + 4x_1x_2 + 4x_2^2
\end{aligned}
\]</span></p>
<p><strong>Step 2: Take partial derivatives</strong> <span class="math inline">\(\frac{\partial}{\partial x_1}(2x_1^2 + 4x_1x_2 + 4x_2^2) = 4x_1 + 4x_2\)</span> <span class="math inline">\(\frac{\partial}{\partial x_2}(2x_1^2 + 4x_1x_2 + 4x_2^2) = 4x_1 + 8x_2\)</span></p>
<p>So <span class="math inline">\(\frac{\partial}{\partial \mathbf{x}}(\mathbf{x}^T \mathbf{A} \mathbf{x}) = \begin{bmatrix} 4x_1 + 4x_2 \\ 4x_1 + 8x_2 \end{bmatrix}\)</span></p>
<p><strong>Step 3: Verify using the formula</strong> <span class="math inline">\(\mathbf{A} + \mathbf{A}^T = \begin{bmatrix} 2 &amp; 1 \\ 3 &amp; 4 \end{bmatrix} + \begin{bmatrix} 2 &amp; 3 \\ 1 &amp; 4 \end{bmatrix} = \begin{bmatrix} 4 &amp; 4 \\ 4 &amp; 8 \end{bmatrix}\)</span></p>
<p><span class="math inline">\((\mathbf{A} + \mathbf{A}^T) \mathbf{x} = \begin{bmatrix} 4 &amp; 4 \\ 4 &amp; 8 \end{bmatrix} \begin{bmatrix} x_1 \\ x_2 \end{bmatrix} = \begin{bmatrix} 4x_1 + 4x_2 \\ 4x_1 + 8x_2 \end{bmatrix}\)</span></p>
<p><strong>Special case - when <span class="math inline">\(\mathbf{A}\)</span> is symmetric:</strong> If <span class="math inline">\(\mathbf{A} = \mathbf{A}^T\)</span> (symmetric matrix), then <span class="math inline">\(\mathbf{A} + \mathbf{A}^T = 2\mathbf{A}\)</span></p>
<p>So <span class="math inline">\(\frac{\partial}{\partial \mathbf{x}}(\mathbf{x}^T \mathbf{A} \mathbf{x}) = 2\mathbf{A} \mathbf{x}\)</span></p>
</section>
<section id="matrix-trace-derivatives" class="level2">
<h2 class="anchored" data-anchor-id="matrix-trace-derivatives">Matrix Trace Derivatives</h2>
<p>The trace of a matrix is the sum of its diagonal elements: <span class="math inline">\(\text{tr}(\mathbf{A}) = A_{11} + A_{22} + \cdots + A_{nn}\)</span></p>
<div class="callout callout-style-default callout-note callout-titled" title="Theorem (Trace Derivatives)">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Theorem (Trace Derivatives)
</div>
</div>
<div class="callout-body-container callout-body">
<p><strong>Formula 1:</strong> <span class="math inline">\(\frac{\partial}{\partial \mathbf{X}} \text{tr}(\mathbf{X}) = \mathbf{I}\)</span></p>
<p><strong>Formula 2:</strong> <span class="math inline">\(\frac{\partial}{\partial \mathbf{X}} \text{tr}(\mathbf{A} \mathbf{X}) = \mathbf{A}^T\)</span></p>
<p><strong>Formula 3:</strong> <span class="math inline">\(\frac{\partial}{\partial \mathbf{X}} \text{tr}(\mathbf{X} \mathbf{A}) = \mathbf{A}^T\)</span></p>
<p><strong>Formula 4:</strong> <span class="math inline">\(\frac{\partial}{\partial \mathbf{X}} \text{tr}(\mathbf{A} \mathbf{X} \mathbf{B}) = \mathbf{A}^T \mathbf{B}^T\)</span></p>
</div>
</div>
<p><strong>Example for Formula 2:</strong> Let <span class="math inline">\(\mathbf{A} = \begin{bmatrix} 1 &amp; 2 \\ 3 &amp; 4 \end{bmatrix}\)</span> and <span class="math inline">\(\mathbf{X} = \begin{bmatrix} x_{11} &amp; x_{12} \\ x_{21} &amp; x_{22} \end{bmatrix}\)</span></p>
<p><span class="math inline">\(\mathbf{A} \mathbf{X} = \begin{bmatrix} 1 &amp; 2 \\ 3 &amp; 4 \end{bmatrix} \begin{bmatrix} x_{11} &amp; x_{12} \\ x_{21} &amp; x_{22} \end{bmatrix} = \begin{bmatrix} x_{11} + 2x_{21} &amp; x_{12} + 2x_{22} \\ 3x_{11} + 4x_{21} &amp; 3x_{12} + 4x_{22} \end{bmatrix}\)</span></p>
<p><span class="math inline">\(\text{tr}(\mathbf{A} \mathbf{X}) = (x_{11} + 2x_{21}) + (3x_{12} + 4x_{22}) = x_{11} + 2x_{21} + 3x_{12} + 4x_{22}\)</span></p>
<p>Taking partial derivatives:</p>
<ul>
<li><span class="math inline">\(\frac{\partial \text{tr}(\mathbf{A} \mathbf{X})}{\partial x_{11}} = 1\)</span></li>
<li><span class="math inline">\(\frac{\partial \text{tr}(\mathbf{A} \mathbf{X})}{\partial x_{12}} = 3\)</span></li>
<li><span class="math inline">\(\frac{\partial \text{tr}(\mathbf{A} \mathbf{X})}{\partial x_{21}} = 2\)</span></li>
<li><span class="math inline">\(\frac{\partial \text{tr}(\mathbf{A} \mathbf{X})}{\partial x_{22}} = 4\)</span></li>
</ul>
<p>So <span class="math inline">\(\frac{\partial}{\partial \mathbf{X}} \text{tr}(\mathbf{A} \mathbf{X}) = \begin{bmatrix} 1 &amp; 3 \\ 2 &amp; 4 \end{bmatrix} = \mathbf{A}^T\)</span></p>
</section>
<section id="determinant-and-inverse-derivatives" class="level2">
<h2 class="anchored" data-anchor-id="determinant-and-inverse-derivatives">Determinant and Inverse Derivatives</h2>
<p>These formulas are more advanced but very important in statistics and optimization.</p>
<div class="callout callout-style-default callout-note callout-titled" title="Theorem (Determinant and Inverse Derivatives)">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Theorem (Determinant and Inverse Derivatives)
</div>
</div>
<div class="callout-body-container callout-body">
<p>For an invertible matrix <span class="math inline">\(\mathbf{X}\)</span>:</p>
<p><strong>Formula 1:</strong> <span class="math inline">\(\frac{\partial}{\partial \mathbf{X}} \det(\mathbf{X}) = \det(\mathbf{X})(\mathbf{X}^{-1})^T\)</span></p>
<p><strong>Formula 2:</strong> <span class="math inline">\(\frac{\partial}{\partial \mathbf{X}} \log \det(\mathbf{X}) = (\mathbf{X}^{-1})^T\)</span></p>
</div>
</div>
<p><strong>What this means:</strong> The derivative of the determinant involves both the determinant itself and the inverse transpose of the matrix.</p>
<p><strong>Applications:</strong> - Maximum likelihood estimation for multivariate normal distributions - Optimization problems involving covariance matrices - Regularization in machine learning</p>
</section>
</section>
<section id="applications-in-machine-learning" class="level1">
<h1>Applications in Machine Learning</h1>
<p>Now let’s see how matrix calculus is used in real machine learning applications.</p>
<p>These examples show why understanding derivatives is crucial for modern AI and data science.</p>
<section id="linear-regression" class="level2">
<h2 class="anchored" data-anchor-id="linear-regression">Linear Regression</h2>
<p>Linear regression is one of the most fundamental machine learning algorithms.</p>
<p>The goal is to find the best line (or hyperplane) that fits through data points.</p>
<p><strong>The Problem:</strong> Given data points <span class="math inline">\((x_1, y_1), (x_2, y_2), \ldots, (x_n, y_n)\)</span>, find the best linear relationship <span class="math inline">\(y \approx \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \cdots + \beta_p x_p\)</span></p>
<p><strong>In matrix form:</strong> We want to solve <span class="math inline">\(\mathbf{y} \approx \mathbf{X} \beta\)</span> where:</p>
<ul>
<li><span class="math inline">\(\mathbf{y} \in \mathbb{R}^n\)</span> is the vector of target values</li>
<li><span class="math inline">\(\mathbf{X} \in \mathbb{R}^{n \times (p+1)}\)</span> is the matrix of input features (with a leading column of ones for the intercept <span class="math inline">\(\beta_0\)</span>)</li>
<li><span class="math inline">\(\beta \in \mathbb{R}^{p+1}\)</span> is the vector of parameters we want to find</li>
</ul>
<p><strong>The objective function:</strong> We minimize the sum of squared errors: minimize <span class="math inline">\(J(\beta) = \|\mathbf{y} - \mathbf{X} \beta\|^2\)</span></p>
<p><strong>Step 1: Expand the objective function</strong> <span class="math inline">\(J(\beta) = (\mathbf{y} - \mathbf{X} \beta)^T (\mathbf{y} - \mathbf{X} \beta)\)</span></p>
<p>Let’s expand this step by step: <span class="math inline">\(J(\beta) = (\mathbf{y}^T - \beta^T \mathbf{X}^T)(\mathbf{y} - \mathbf{X} \beta) = \mathbf{y}^T \mathbf{y} - \mathbf{y}^T \mathbf{X} \beta - \beta^T \mathbf{X}^T \mathbf{y} + \beta^T \mathbf{X}^T \mathbf{X} \beta\)</span></p>
<p><strong>Note:</strong> <span class="math inline">\(\mathbf{y}^T \mathbf{X} \beta\)</span> is a scalar, so <span class="math inline">\(\mathbf{y}^T \mathbf{X} \beta = (\mathbf{y}^T \mathbf{X} \beta)^T = \beta^T \mathbf{X}^T \mathbf{y}\)</span></p>
<p>Therefore: <span class="math inline">\(J(\beta) = \mathbf{y}^T \mathbf{y} - 2 \beta^T \mathbf{X}^T \mathbf{y} + \beta^T \mathbf{X}^T \mathbf{X} \beta\)</span></p>
<p><strong>Step 2: Take the gradient with respect to <span class="math inline">\(\beta\)</span></strong> Using our derivative formulas:</p>
<ul>
<li><span class="math inline">\(\frac{\partial}{\partial \beta}(\mathbf{y}^T \mathbf{y}) = \mathbf{0}\)</span> (constant with respect to <span class="math inline">\(\beta\)</span>)</li>
<li><span class="math inline">\(\frac{\partial}{\partial \beta}(-2 \beta^T \mathbf{X}^T \mathbf{y}) = -2 \mathbf{X}^T \mathbf{y}\)</span> (linear form)</li>
<li><span class="math inline">\(\frac{\partial}{\partial \beta}(\beta^T (\mathbf{X}^T \mathbf{X}) \beta) = 2 \mathbf{X}^T \mathbf{X} \beta\)</span> (quadratic form with symmetric matrix <span class="math inline">\(\mathbf{X}^T \mathbf{X}\)</span>)</li>
</ul>
<p>So: <span class="math inline">\(\nabla J = \frac{\partial J}{\partial \beta} = -2 \mathbf{X}^T \mathbf{y} + 2 \mathbf{X}^T \mathbf{X} \beta\)</span></p>
<p><strong>Step 3: Set the gradient to zero and solve</strong> <span class="math inline">\(\nabla J = \mathbf{0}\)</span> <span class="math inline">\(-2 \mathbf{X}^T \mathbf{y} + 2 \mathbf{X}^T \mathbf{X} \beta = \mathbf{0}\)</span> <span class="math inline">\(\mathbf{X}^T \mathbf{X} \beta = \mathbf{X}^T \mathbf{y}\)</span> <span class="math inline">\(\beta^* = (\mathbf{X}^T \mathbf{X})^{-1} \mathbf{X}^T \mathbf{y}\)</span></p>
<p><strong>This is the famous normal equation for linear regression!</strong></p>
<p><strong>What this means:</strong> The optimal parameters <span class="math inline">\(\beta^*\)</span> can be computed directly using matrix operations.</p>
<p>No iterative optimization needed - just matrix multiplication and inversion.</p>
<p><strong>Geometric interpretation:</strong> The vector <span class="math inline">\(\mathbf{X}\beta^*\)</span> is the orthogonal projection of <span class="math inline">\(\mathbf{y}\)</span> onto the column space of <span class="math inline">\(\mathbf{X}\)</span>.</p>
<p><strong>Practical note:</strong> In practice, we often use QR decomposition or SVD instead of computing <span class="math inline">\((\mathbf{X}^T \mathbf{X})^{-1}\)</span> directly for numerical stability.</p>
</section>
<section id="principal-component-analysis-pca" class="level2">
<h2 class="anchored" data-anchor-id="principal-component-analysis-pca">Principal Component Analysis (PCA)</h2>
<p>PCA is a dimensionality reduction technique that finds the directions of maximum variance in data.</p>
<p><strong>The Problem:</strong> Given data points in high-dimensional space, find a lower-dimensional representation that preserves as much information as possible.</p>
<p><strong>Mathematical formulation:</strong> Find the direction <span class="math inline">\(\mathbf{w}\)</span> (unit vector) that maximizes the variance of the data when projected onto <span class="math inline">\(\mathbf{w}\)</span>.</p>
<p><strong>The optimization problem:</strong> maximize <span class="math inline">\(\mathbf{w}^T \mathbf{S} \mathbf{w}\)</span> subject to <span class="math inline">\(\mathbf{w}^T \mathbf{w} = 1\)</span></p>
<p>where <span class="math inline">\(\mathbf{S}\)</span> is the sample covariance matrix of the data.</p>
<p><strong>Step 1: Set up the Lagrangian</strong> We use the method of Lagrange multipliers to handle the constraint.</p>
<p><span class="math inline">\(L(\mathbf{w}, \lambda) = \mathbf{w}^T \mathbf{S} \mathbf{w} - \lambda (\mathbf{w}^T \mathbf{w} - 1)\)</span></p>
<p>where <span class="math inline">\(\lambda\)</span> is the Lagrange multiplier.</p>
<p><strong>Step 2: Take the gradient with respect to <span class="math inline">\(\mathbf{w}\)</span></strong> <span class="math inline">\(\frac{\partial L}{\partial \mathbf{w}} = \frac{\partial}{\partial \mathbf{w}}(\mathbf{w}^T \mathbf{S} \mathbf{w}) - \lambda \frac{\partial}{\partial \mathbf{w}}(\mathbf{w}^T \mathbf{w})\)</span></p>
<p>Using our quadratic form formulas:</p>
<ul>
<li><span class="math inline">\(\frac{\partial}{\partial \mathbf{w}}(\mathbf{w}^T \mathbf{S} \mathbf{w}) = (\mathbf{S} + \mathbf{S}^T) \mathbf{w} = 2 \mathbf{S} \mathbf{w}\)</span> (since <span class="math inline">\(\mathbf{S}\)</span> is symmetric)</li>
<li><span class="math inline">\(\frac{\partial}{\partial \mathbf{w}}(\mathbf{w}^T \mathbf{w}) = 2 \mathbf{w}\)</span></li>
</ul>
<p>So: <span class="math inline">\(\frac{\partial L}{\partial \mathbf{w}} = 2 \mathbf{S} \mathbf{w} - 2 \lambda \mathbf{w}\)</span></p>
<p><strong>Step 3: Set the gradient to zero</strong> <span class="math inline">\(2 \mathbf{S} \mathbf{w} - 2 \lambda \mathbf{w} = \mathbf{0}\)</span> <span class="math inline">\(\mathbf{S} \mathbf{w} = \lambda \mathbf{w}\)</span></p>
<p><strong>This is the eigenvalue equation!</strong></p>
<p><strong>What this means:</strong> The optimal direction <span class="math inline">\(\mathbf{w}\)</span> is an eigenvector of the covariance matrix <span class="math inline">\(\mathbf{S}\)</span>.</p>
<p>The corresponding eigenvalue <span class="math inline">\(\lambda\)</span> tells us how much variance is captured in that direction.</p>
<p><strong>Complete solution:</strong></p>
<ul>
<li>The first principal component is the eigenvector with the largest eigenvalue</li>
<li>The second principal component is the eigenvector with the second largest eigenvalue</li>
<li>And so on…</li>
</ul>
<p><strong>Geometric interpretation:</strong> PCA finds the axes along which the data varies the most.</p>
<p><strong>Physical interpretation:</strong> If the data represents physical measurements, PCA finds the “natural coordinate system” of the data.</p>
</section>
<section id="neural-network-backpropagation" class="level2">
<h2 class="anchored" data-anchor-id="neural-network-backpropagation">Neural Network Backpropagation</h2>
<p>Backpropagation is the algorithm used to train neural networks.</p>
<p>It’s essentially a systematic application of the chain rule to compute gradients efficiently.</p>
<p><strong>Simple network example:</strong> Consider a single layer: <span class="math inline">\(\mathbf{z} = \mathbf{W} \mathbf{x} + \mathbf{b}\)</span>, <span class="math inline">\(\mathbf{a} = \sigma(\mathbf{z})\)</span></p>
<p>where:</p>
<ul>
<li><span class="math inline">\(\mathbf{x} \in \mathbb{R}^n\)</span> is the input vector</li>
<li><span class="math inline">\(\mathbf{W} \in \mathbb{R}^{m \times n}\)</span> is the weight matrix</li>
<li><span class="math inline">\(\mathbf{b} \in \mathbb{R}^m\)</span> is the bias vector</li>
<li><span class="math inline">\(\mathbf{z} \in \mathbb{R}^m\)</span> is the pre-activation</li>
<li><span class="math inline">\(\sigma\)</span> is an activation function (applied element-wise)</li>
<li><span class="math inline">\(\mathbf{a} \in \mathbb{R}^m\)</span> is the activation (output)</li>
</ul>
<p><strong>The goal:</strong> Compute <span class="math inline">\(\frac{\partial L}{\partial \mathbf{W}}\)</span> where <span class="math inline">\(L\)</span> is some scalar loss function.</p>
<p><strong>Step 1: Apply the chain rule</strong> We can think of this as <span class="math inline">\(\frac{\partial L}{\partial \mathbf{W}} = \frac{\partial L}{\partial \mathbf{z}} \frac{\partial \mathbf{z}}{\partial \mathbf{W}}\)</span>.</p>
<p><strong>Step 2: Compute each term</strong></p>
<p>The term <span class="math inline">\(\frac{\partial L}{\partial \mathbf{z}}\)</span> represents the gradient of the loss with respect to the pre-activations. It is computed from upstream layers. Let’s call this vector <span class="math inline">\(\delta = \frac{\partial L}{\partial \mathbf{z}} \in \mathbb{R}^m\)</span>.</p>
<p>The term <span class="math inline">\(\frac{\partial \mathbf{z}}{\partial \mathbf{W}}\)</span> is more complex. Let’s find the partial derivative of an element <span class="math inline">\(z_i\)</span> with respect to an element <span class="math inline">\(W_{ij}\)</span>. Since <span class="math inline">\(z_i = \sum_k W_{ik} x_k + b_i\)</span>, we have <span class="math inline">\(\frac{\partial z_i}{\partial W_{jk}} = \delta_{ij} x_k\)</span>, where <span class="math inline">\(\delta_{ij}\)</span> is the Kronecker delta.</p>
<p><strong>Step 3: Combine using matrix operations</strong> A more direct approach is to consider the contribution of each weight <span class="math inline">\(W_{ij}\)</span> to the loss <span class="math inline">\(L\)</span>. <span class="math display">\[\frac{\partial L}{\partial W_{ij}} = \frac{\partial L}{\partial z_i} \frac{\partial z_i}{\partial W_{ij}} = \delta_i x_j\]</span> If we arrange these into a matrix, we get: <span class="math display">\[\frac{\partial L}{\partial \mathbf{W}} = \delta \mathbf{x}^T\]</span> where <span class="math inline">\(\delta = \frac{\partial L}{\partial \mathbf{z}} \in \mathbb{R}^m\)</span> and <span class="math inline">\(\mathbf{x}^T \in \mathbb{R}^{1 \times n}\)</span>, resulting in an <span class="math inline">\(m \times n\)</span> matrix, which is the correct shape for the gradient with respect to <span class="math inline">\(\mathbf{W}\)</span>.</p>
<p><strong>Key insight:</strong> This gradient computation can be done efficiently for networks with millions of parameters by systematically applying the chain rule layer by layer.</p>
<p><strong>Why this works:</strong> The chain rule allows us to decompose the complex dependency of the loss on the weights into simple, local computations.</p>
</section>
</section>
<section id="advanced-topics-and-extensions" class="level1">
<h1>Advanced Topics and Extensions</h1>
<p>Now let’s explore some more advanced concepts that build on the foundation we’ve established.</p>
<section id="higher-order-derivatives-the-hessian-matrix" class="level2">
<h2 class="anchored" data-anchor-id="higher-order-derivatives-the-hessian-matrix">Higher-Order Derivatives: The Hessian Matrix</h2>
<p>Just as we can take second derivatives in single-variable calculus, we can take second derivatives in matrix calculus.</p>
<p>The most important second-order derivative is the Hessian matrix.</p>
<div class="callout callout-style-default callout-note callout-titled" title="Definition">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Definition
</div>
</div>
<div class="callout-body-container callout-body">
<p>The <strong>Hessian matrix</strong> of a scalar function <span class="math inline">\(f: \mathbb{R}^n \to \mathbb{R}\)</span> is:</p>
<p><span class="math display">\[\mathbf{H} = \frac{\partial^2 f}{\partial \mathbf{x} \partial \mathbf{x}^T} = \left[ \frac{\partial^2 f}{\partial x_i \partial x_j} \right] \in \mathbb{R}^{n \times n}\]</span></p>
<p><strong>What this means:</strong> The Hessian is a square matrix where each element <span class="math inline">\((i,j)\)</span> is the second partial derivative <span class="math inline">\(\frac{\partial^2 f}{\partial x_i \partial x_j}\)</span>.</p>
<p><strong>Element-by-element:</strong> <span class="math inline">\(H_{ij} = \frac{\partial^2 f}{\partial x_i \partial x_j}\)</span></p>
<p>This tells us how the gradient in the <span class="math inline">\(i\)</span>-th direction changes when we move in the <span class="math inline">\(j\)</span>-th direction.</p>
<p><strong>Important property:</strong> If <span class="math inline">\(f\)</span> is twice continuously differentiable, then the Hessian is symmetric: <span class="math inline">\(\mathbf{H} = \mathbf{H}^T\)</span>. This is because <span class="math inline">\(\frac{\partial^2 f}{\partial x_i \partial x_j} = \frac{\partial^2 f}{\partial x_j \partial x_i}\)</span> (Clairaut’s theorem).</p>
</div>
</div>
<p><strong>Detailed Example:</strong> Let <span class="math inline">\(f(\mathbf{x}) = \mathbf{x}^T \mathbf{A} \mathbf{x}\)</span> where <span class="math inline">\(\mathbf{A}\)</span> is a symmetric matrix.</p>
<p><strong>Step 1: Find the gradient</strong> From our earlier formulas: <span class="math inline">\(\nabla f = 2 \mathbf{A} \mathbf{x}\)</span></p>
<p><strong>Step 2: Find the Hessian</strong> The Hessian is the Jacobian of the gradient vector: <span class="math inline">\(\mathbf{H} = \frac{\partial}{\partial \mathbf{x}^T} (\nabla f) = \frac{\partial}{\partial \mathbf{x}^T} (2\mathbf{A}\mathbf{x})\)</span> Using the linear form rule <span class="math inline">\(\frac{\partial}{\partial \mathbf{x}}(\mathbf{M}\mathbf{x}) = \mathbf{M}^T\)</span> and adapting for our layout convention: <span class="math inline">\(\mathbf{H} = 2\mathbf{A}\)</span></p>
<p><strong>General result:</strong> For <span class="math inline">\(f(\mathbf{x}) = \mathbf{x}^T \mathbf{A} \mathbf{x}\)</span> where <span class="math inline">\(\mathbf{A}\)</span> is symmetric, the Hessian is <span class="math inline">\(\mathbf{H} = 2 \mathbf{A}\)</span>.</p>
<section id="applications-of-the-hessian" class="level3">
<h3 class="anchored" data-anchor-id="applications-of-the-hessian">Applications of the Hessian</h3>
<p><strong>Newton’s Method for Optimization:</strong> To find the minimum of <span class="math inline">\(f(\mathbf{x})\)</span>, Newton’s method uses both first and second derivatives:</p>
<p><span class="math inline">\(\mathbf{x}_{k+1} = \mathbf{x}_k - \mathbf{H}(\mathbf{x}_k)^{-1} \nabla f(\mathbf{x}_k)\)</span></p>
<p><strong>Geometric interpretation:</strong> Newton’s method fits a quadratic approximation to the function at each step and jumps to the minimum of that quadratic.</p>
<p><strong>Convergence properties:</strong> Newton’s method converges much faster than gradient descent when near the optimum, but requires computing and inverting the Hessian.</p>
<p><strong>Analyzing Critical Points:</strong> When <span class="math inline">\(\nabla f(\mathbf{x}^*) = \mathbf{0}\)</span> (critical point), the eigenvalues of <span class="math inline">\(\mathbf{H}(\mathbf{x}^*)\)</span> tell us about the nature of the critical point:</p>
<ul>
<li><strong>All eigenvalues positive:</strong> <span class="math inline">\(\mathbf{x}^*\)</span> is a local minimum (the function curves upward in all directions)</li>
<li><strong>All eigenvalues negative:</strong> <span class="math inline">\(\mathbf{x}^*\)</span> is a local maximum (the function curves downward in all directions)</li>
<li><strong>Mixed eigenvalues:</strong> <span class="math inline">\(\mathbf{x}^*\)</span> is a saddle point (curves up in some directions, down in others)</li>
</ul>
<p><strong>Quadratic Approximations:</strong> Near a point <span class="math inline">\(\mathbf{a}\)</span>, we can approximate <span class="math inline">\(f\)</span> using the Taylor expansion:</p>
<p><span class="math inline">\(f(\mathbf{x}) \approx f(\mathbf{a}) + \nabla f(\mathbf{a})^T (\mathbf{x} - \mathbf{a}) + \frac{1}{2} (\mathbf{x} - \mathbf{a})^T \mathbf{H}(\mathbf{a}) (\mathbf{x} - \mathbf{a})\)</span></p>
<p><strong>What each term means:</strong></p>
<ul>
<li><span class="math inline">\(f(\mathbf{a})\)</span>: the function value at the expansion point</li>
<li><span class="math inline">\(\nabla f(\mathbf{a})^T (\mathbf{x} - \mathbf{a})\)</span>: first-order (linear) change</li>
<li><span class="math inline">\(\frac{1}{2} (\mathbf{x} - \mathbf{a})^T \mathbf{H}(\mathbf{a}) (\mathbf{x} - \mathbf{a})\)</span>: second-order (quadratic) change</li>
</ul>
</section>
</section>
<section id="matrix-differential-calculus" class="level2">
<h2 class="anchored" data-anchor-id="matrix-differential-calculus">Matrix Differential Calculus</h2>
<p>An alternative approach to matrix calculus uses matrix differentials.</p>
<p>This can be more intuitive for some calculations, especially when dealing with complex matrix expressions.</p>
<div class="callout callout-style-default callout-note callout-titled" title="Definition">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Definition
</div>
</div>
<div class="callout-body-container callout-body">
<p>The <strong>differential</strong> of a function <span class="math inline">\(f(\mathbf{X})\)</span> is defined by the first-order approximation: <span class="math inline">\(f(\mathbf{X} + d\mathbf{X}) - f(\mathbf{X}) \approx df\)</span> where <span class="math inline">\(df\)</span> is a linear function of <span class="math inline">\(d\mathbf{X}\)</span>. The relationship between the differential and the gradient is <span class="math inline">\(df = \text{tr}\left( \left(\frac{\partial f}{\partial \mathbf{X}}\right)^T d\mathbf{X} \right)\)</span>.</p>
</div>
</div>
<p><strong>What this means:</strong> Instead of computing partial derivatives directly, we find a linear approximation for how small changes in <span class="math inline">\(\mathbf{X}\)</span> (represented by <span class="math inline">\(d\mathbf{X}\)</span>) affect <span class="math inline">\(f\)</span>.</p>
<p><strong>Basic differential rules:</strong></p>
<p><strong>Linearity:</strong> <span class="math inline">\(d(\alpha \mathbf{A} + \beta \mathbf{B}) = \alpha d\mathbf{A} + \beta d\mathbf{B}\)</span></p>
<p><strong>Product rule:</strong> <span class="math inline">\(d(\mathbf{A} \mathbf{B}) = (d\mathbf{A}) \mathbf{B} + \mathbf{A} (d\mathbf{B})\)</span></p>
<p><strong>This is the matrix version of the familiar product rule from calculus.</strong></p>
<p><strong>Inverse rule:</strong> <span class="math inline">\(d(\mathbf{A}^{-1}) = -\mathbf{A}^{-1} (d\mathbf{A}) \mathbf{A}^{-1}\)</span></p>
<p><strong>Example derivation:</strong> Since <span class="math inline">\(\mathbf{A} \mathbf{A}^{-1} = \mathbf{I}\)</span>, taking differentials: <span class="math inline">\(d(\mathbf{A} \mathbf{A}^{-1}) = d(\mathbf{I}) = \mathbf{0}\)</span></p>
<p>Using the product rule: <span class="math inline">\((d\mathbf{A}) \mathbf{A}^{-1} + \mathbf{A} (d\mathbf{A}^{-1}) = \mathbf{0}\)</span></p>
<p>Solving for <span class="math inline">\(d\mathbf{A}^{-1}\)</span>: <span class="math inline">\(\mathbf{A} (d\mathbf{A}^{-1}) = -(d\mathbf{A}) \mathbf{A}^{-1}\)</span> <span class="math inline">\(d\mathbf{A}^{-1} = -\mathbf{A}^{-1} (d\mathbf{A}) \mathbf{A}^{-1}\)</span></p>
<p><strong>Trace rule:</strong> <span class="math inline">\(d(\text{tr}(\mathbf{A})) = \text{tr}(d\mathbf{A})\)</span></p>
<p><strong>Determinant rule:</strong> <span class="math inline">\(d(\det(\mathbf{A})) = \det(\mathbf{A}) \text{tr}(\mathbf{A}^{-1} d\mathbf{A})\)</span></p>
</section>
<section id="vectorization-and-kronecker-products" class="level2">
<h2 class="anchored" data-anchor-id="vectorization-and-kronecker-products">Vectorization and Kronecker Products</h2>
<p>When dealing with complex matrix derivatives, vectorization becomes a powerful tool.</p>
<div class="callout callout-style-default callout-note callout-titled" title="Definition">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Definition
</div>
</div>
<div class="callout-body-container callout-body">
<p>The <strong>vectorization</strong> operator <span class="math inline">\(\text{vec}(\mathbf{A})\)</span> stacks the columns of matrix <span class="math inline">\(\mathbf{A}\)</span> into a single column vector.</p>
</div>
</div>
<p><strong>Example:</strong> If <span class="math inline">\(\mathbf{A} = \begin{bmatrix} 1 &amp; 3 \\ 2 &amp; 4 \end{bmatrix}\)</span>, then <span class="math inline">\(\text{vec}(\mathbf{A}) = \begin{bmatrix} 1 \\ 2 \\ 3 \\ 4 \end{bmatrix}\)</span></p>
<p><strong>Key property:</strong> Vectorization converts matrix equations into vector equations, which are often easier to manipulate.</p>
<div class="callout callout-style-default callout-note callout-titled" title="Definition">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Definition
</div>
</div>
<div class="callout-body-container callout-body">
<p>The <strong>Kronecker product</strong> <span class="math inline">\(\mathbf{A} \otimes \mathbf{B}\)</span> is defined as:</p>
<p><span class="math display">\[\mathbf{A} \otimes \mathbf{B} = \begin{bmatrix} a_{11} \mathbf{B} &amp; a_{12} \mathbf{B} &amp; \cdots \\ a_{21} \mathbf{B} &amp; a_{22} \mathbf{B} &amp; \cdots \\ \vdots &amp; \vdots &amp; \ddots \end{bmatrix}\]</span></p>
<p><strong>Example:</strong> <span class="math inline">\(\begin{bmatrix} 1 &amp; 2 \\ 3 &amp; 4 \end{bmatrix} \otimes \begin{bmatrix} 5 &amp; 6 \\ 7 &amp; 8 \end{bmatrix} = \begin{bmatrix} 1 \cdot \begin{bmatrix} 5 &amp; 6 \\ 7 &amp; 8 \end{bmatrix} &amp; 2 \cdot \begin{bmatrix} 5 &amp; 6 \\ 7 &amp; 8 \end{bmatrix} \\ 3 \cdot \begin{bmatrix} 5 &amp; 6 \\ 7 &amp; 8 \end{bmatrix} &amp; 4 \cdot \begin{bmatrix} 5 &amp; 6 \\ 7 &amp; 8 \end{bmatrix} \end{bmatrix} = \begin{bmatrix} 5 &amp; 6 &amp; 10 &amp; 12 \\ 7 &amp; 8 &amp; 14 &amp; 16 \\ 15 &amp; 18 &amp; 20 &amp; 24 \\ 21 &amp; 24 &amp; 28 &amp; 32 \end{bmatrix}\)</span></p>
</div>
</div>
<div class="callout callout-style-default callout-note callout-titled" title="Theorem (Vectorization Identity)">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Theorem (Vectorization Identity)
</div>
</div>
<div class="callout-body-container callout-body">
<p>For matrices <span class="math inline">\(\mathbf{A}\)</span>, <span class="math inline">\(\mathbf{B}\)</span>, <span class="math inline">\(\mathbf{C}\)</span> of appropriate dimensions:</p>
<p><span class="math display">\[\text{vec}(\mathbf{A} \mathbf{B} \mathbf{C}) = (\mathbf{C}^T \otimes \mathbf{A}) \text{vec}(\mathbf{B})\]</span></p>
</div>
</div>
<p><strong>What this means:</strong> Matrix multiplication can be expressed as a linear transformation of the vectorized middle matrix.</p>
<p><strong>Applications:</strong> This identity is crucial for:</p>
<ul>
<li>Converting matrix equations into vector form for optimization</li>
<li>Deriving gradients for complex matrix expressions</li>
<li>Solving matrix equations using vector methods</li>
</ul>
<p><strong>Example application:</strong> To solve the matrix equation <span class="math inline">\(\mathbf{A} \mathbf{X} \mathbf{B} = \mathbf{C}\)</span> for <span class="math inline">\(\mathbf{X}\)</span>:</p>
<p><strong>Step 1:</strong> Vectorize both sides <span class="math inline">\(\text{vec}(\mathbf{A} \mathbf{X} \mathbf{B}) = \text{vec}(\mathbf{C})\)</span></p>
<p><strong>Step 2:</strong> Apply the vectorization identity <span class="math inline">\((\mathbf{B}^T \otimes \mathbf{A}) \text{vec}(\mathbf{X}) = \text{vec}(\mathbf{C})\)</span></p>
<p><strong>Step 3:</strong> Solve the linear system <span class="math inline">\(\text{vec}(\mathbf{X}) = (\mathbf{B}^T \otimes \mathbf{A})^{-1} \text{vec}(\mathbf{C})\)</span></p>
<p><strong>Step 4:</strong> Reshape back to matrix form <span class="math inline">\(\mathbf{X} = \text{reshape}(\text{vec}(\mathbf{X}), \text{original dimensions})\)</span></p>
</section>
</section>
<section id="computational-considerations" class="level1">
<h1>Computational Considerations</h1>
<p>Understanding the theory is only half the battle - implementing matrix calculus efficiently and accurately is crucial for practical applications.</p>
<section id="automatic-differentiation-autodiff" class="level2">
<h2 class="anchored" data-anchor-id="automatic-differentiation-autodiff">Automatic Differentiation (Autodiff)</h2>
<p>Modern machine learning frameworks like TensorFlow, PyTorch, and JAX use automatic differentiation to compute derivatives.</p>
<p>This is different from both symbolic differentiation (like Mathematica) and numerical differentiation (finite differences).</p>
<p><strong>The key insight:</strong> Every computer program, no matter how complex, is built from elementary operations (<span class="math inline">\(+\)</span>, <span class="math inline">\(-\)</span>, <span class="math inline">\(\times\)</span>, <span class="math inline">\(\div\)</span>, <span class="math inline">\(\exp\)</span>, <span class="math inline">\(\log\)</span>, <span class="math inline">\(\sin\)</span>, <span class="math inline">\(\cos\)</span>, etc.).</p>
<p>If we know the derivatives of these elementary operations, we can use the chain rule to compute derivatives of arbitrarily complex programs.</p>
<section id="forward-mode-autodiff" class="level3">
<h3 class="anchored" data-anchor-id="forward-mode-autodiff">Forward Mode Autodiff</h3>
<p><strong>How it works:</strong> We compute derivatives alongside the original computation, moving forward through the computation graph.</p>
<p><strong>Example:</strong> Suppose we want to compute <span class="math inline">\(f(x_1, x_2) = \sin(x_1) + x_1x_2\)</span> and its partial derivatives.</p>
<p><strong>Forward pass (computing function and derivatives simultaneously):</strong></p>
<p><strong>Step 1:</strong> Input values and seed derivatives</p>
<ul>
<li><span class="math inline">\(x_1 = 2\)</span>, <span class="math inline">\(\dot{x}_1 = 1\)</span> (for <span class="math inline">\(\frac{\partial}{\partial x_1}\)</span>)</li>
<li><span class="math inline">\(x_2 = 3\)</span>, <span class="math inline">\(\dot{x}_2 = 0\)</span></li>
</ul>
<p><strong>Step 2:</strong> Compute <span class="math inline">\(\sin(x_1)\)</span></p>
<ul>
<li><span class="math inline">\(v_1 = \sin(x_1) = \sin(2) \approx 0.909\)</span></li>
<li><span class="math inline">\(\dot{v}_1 = \cos(x_1) \cdot \dot{x}_1 = \cos(2) \cdot 1 \approx -0.416\)</span></li>
</ul>
<p><strong>Step 3:</strong> Compute <span class="math inline">\(x_1x_2\)</span></p>
<ul>
<li><span class="math inline">\(v_2 = x_1x_2 = 2 \cdot 3 = 6\)</span></li>
<li><span class="math inline">\(\dot{v}_2 = \dot{x}_1 x_2 + x_1 \dot{x}_2 = 1 \cdot 3 + 2 \cdot 0 = 3\)</span></li>
</ul>
<p><strong>Step 4:</strong> Compute <span class="math inline">\(v_1 + v_2\)</span></p>
<ul>
<li><span class="math inline">\(f = v_1 + v_2 = 0.909 + 6 = 6.909\)</span></li>
<li><span class="math inline">\(\dot{f} = \dot{v}_1 + \dot{v}_2 = -0.416 + 3 = 2.584\)</span> (This is <span class="math inline">\(\frac{\partial f}{\partial x_1}\)</span>)</li>
</ul>
<p>To get <span class="math inline">\(\frac{\partial f}{\partial x_2}\)</span>, we need another pass with <span class="math inline">\(\dot{x}_1 = 0, \dot{x}_2 = 1\)</span>.</p>
<p><strong>When forward mode is efficient:</strong> Forward mode is efficient when the number of inputs is much smaller than the number of outputs.</p>
<p>Example: <span class="math inline">\(f: \mathbb{R}^2 \to \mathbb{R}^{1000}\)</span> (2 inputs, 1000 outputs)</p>
<ul>
<li>Forward mode: 2 passes (one for each input)</li>
<li>Reverse mode: 1000 passes (one for each output)</li>
</ul>
</section>
<section id="reverse-mode-autodiff-backpropagation" class="level3">
<h3 class="anchored" data-anchor-id="reverse-mode-autodiff-backpropagation">Reverse Mode Autodiff (Backpropagation)</h3>
<p><strong>How it works:</strong> We first compute the function value in a forward pass, then compute derivatives in a backward pass through the computation graph.</p>
<p><strong>The key idea:</strong> Use the chain rule systematically, starting from the output and working backward.</p>
<p><strong>Example (same function):</strong> <span class="math inline">\(f(x_1, x_2) = \sin(x_1) + x_1x_2\)</span></p>
<p><strong>Forward pass (compute function only):</strong></p>
<ul>
<li><span class="math inline">\(x_1 = 2, x_2 = 3\)</span></li>
<li><span class="math inline">\(v_1 = \sin(x_1) \approx 0.909\)</span></li>
<li><span class="math inline">\(v_2 = x_1x_2 = 6\)</span></li>
<li><span class="math inline">\(f = v_1 + v_2 = 6.909\)</span></li>
</ul>
<p><strong>Backward pass (compute derivatives):</strong></p>
<p><strong>Step 1:</strong> Start with the “seed” <span class="math inline">\(\bar{f} = \frac{\partial f}{\partial f} = 1\)</span></p>
<p><strong>Step 2:</strong> Backpropagate through addition (<span class="math inline">\(f = v_1 + v_2\)</span>)</p>
<ul>
<li><span class="math inline">\(\bar{v}_1 = \bar{f} \frac{\partial f}{\partial v_1} = 1 \cdot 1 = 1\)</span></li>
<li><span class="math inline">\(\bar{v}_2 = \bar{f} \frac{\partial f}{\partial v_2} = 1 \cdot 1 = 1\)</span></li>
</ul>
<p><strong>Step 3:</strong> Backpropagate through <span class="math inline">\(\sin\)</span> (<span class="math inline">\(v_1 = \sin(x_1)\)</span>) and product (<span class="math inline">\(v_2 = x_1x_2\)</span>)</p>
<ul>
<li><span class="math inline">\(\bar{x}_1 = \bar{v}_1 \frac{\partial v_1}{\partial x_1} + \bar{v}_2 \frac{\partial v_2}{\partial x_1} = 1 \cdot \cos(x_1) + 1 \cdot x_2 = \cos(2) + 3 \approx 2.584\)</span></li>
<li><span class="math inline">\(\bar{x}_2 = \bar{v}_2 \frac{\partial v_2}{\partial x_2} = 1 \cdot x_1 = 2\)</span></li>
</ul>
<p>The results are <span class="math inline">\(\frac{\partial f}{\partial x_1} = \bar{x}_1\)</span> and <span class="math inline">\(\frac{\partial f}{\partial x_2} = \bar{x}_2\)</span>.</p>
<p><strong>When reverse mode is efficient:</strong> Reverse mode is efficient when the number of outputs is much smaller than the number of inputs.</p>
<p>Example: <span class="math inline">\(f: \mathbb{R}^{10^6} \to \mathbb{R}\)</span> (1 million inputs, 1 output)</p>
<ul>
<li>Forward mode: 1,000,000 passes</li>
<li>Reverse mode: 1 pass</li>
</ul>
<p><strong>This is why neural networks can be trained efficiently!</strong></p>
<div class="callout callout-style-default callout-note callout-titled" title="Key Point">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Key Point
</div>
</div>
<div class="callout-body-container callout-body">
<p>Reverse mode autodiff is why we can efficiently train neural networks with millions of parameters.</p>
<p>It computes all partial derivatives of a scalar loss function in time roughly proportional to the forward pass.</p>
<p>Without this, modern deep learning would be computationally infeasible.</p>
</div>
</div>
</section>
</section>
<section id="numerical-stability" class="level2">
<h2 class="anchored" data-anchor-id="numerical-stability">Numerical Stability</h2>
<p>When implementing matrix calculus operations, numerical errors can accumulate and cause serious problems.</p>
<p>Here are the most important considerations:</p>
<section id="matrix-inversion" class="level3">
<h3 class="anchored" data-anchor-id="matrix-inversion">Matrix Inversion</h3>
<p><strong>The problem:</strong> Never compute <span class="math inline">\((\mathbf{X}^T\mathbf{X})^{-1}\)</span> directly when <span class="math inline">\(\mathbf{X}\)</span> is tall and thin (more rows than columns).</p>
<p><strong>Why this fails:</strong> The condition number of <span class="math inline">\(\mathbf{X}^T\mathbf{X}\)</span> is the square of the condition number of <span class="math inline">\(\mathbf{X}\)</span>.</p>
<p>If <span class="math inline">\(\mathbf{X}\)</span> is slightly ill-conditioned, <span class="math inline">\(\mathbf{X}^T\mathbf{X}\)</span> becomes very ill-conditioned.</p>
<p><strong>The solution:</strong> Use QR decomposition instead to solve the linear system <span class="math inline">\((\mathbf{X}^T\mathbf{X})\beta = \mathbf{X}^T\mathbf{y}\)</span>.</p>
<p><strong>QR decomposition:</strong> Any matrix <span class="math inline">\(\mathbf{X}\)</span> can be written as <span class="math inline">\(\mathbf{X} = \mathbf{Q}\mathbf{R}\)</span> where:</p>
<ul>
<li><span class="math inline">\(\mathbf{Q}\)</span> has orthonormal columns (<span class="math inline">\(\mathbf{Q}^T\mathbf{Q} = \mathbf{I}\)</span>)</li>
<li><span class="math inline">\(\mathbf{R}\)</span> is upper triangular</li>
</ul>
<p>The system becomes <span class="math inline">\(\mathbf{R}^T\mathbf{Q}^T\mathbf{Q}\mathbf{R}\beta = \mathbf{R}^T\mathbf{Q}^T\mathbf{y}\)</span>, which simplifies to <span class="math inline">\(\mathbf{R}\beta = \mathbf{Q}^T\mathbf{y}\)</span>. This can be solved efficiently and stably using back-substitution.</p>
</section>
<section id="log-sum-exp-trick" class="level3">
<h3 class="anchored" data-anchor-id="log-sum-exp-trick">Log-Sum-Exp Trick</h3>
<p><strong>The problem:</strong> Computing <span class="math inline">\(\log(\exp(a_1) + \exp(a_2) + \cdots + \exp(a_n))\)</span> directly can cause overflow or underflow.</p>
<p><strong>Example:</strong> If <span class="math inline">\(a_1 = 1000\)</span>, then <span class="math inline">\(\exp(1000)\)</span> is astronomically large and will overflow.</p>
<p><strong>The solution:</strong> Factor out the maximum value: Let <span class="math inline">\(a_{\max} = \max_i a_i\)</span>.</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode latex code-with-copy"><code class="sourceCode latex"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="kw">\begin</span>{<span class="ex">aligned</span>}</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="fu">\log</span>(<span class="fu">\sum</span>_i <span class="fu">\exp</span>(a_i)) &amp;= <span class="fu">\log</span>(<span class="fu">\exp</span>(a_{<span class="fu">\max</span>})<span class="fu">\sum</span>_i <span class="fu">\exp</span>(a_i - a_{<span class="fu">\max</span>})) <span class="fu">\\</span></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a>&amp;= a_{<span class="fu">\max</span>} + <span class="fu">\log</span>(<span class="fu">\sum</span>_i <span class="fu">\exp</span>(a_i - a_{<span class="fu">\max</span>}))</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="kw">\end</span>{<span class="ex">aligned</span>}</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p><strong>Why this works:</strong> All the terms <span class="math inline">\(\exp(a_i - a_{\max})\)</span> are <span class="math inline">\(\le 1\)</span>, so no overflow. At least one term equals 1, so the sum is at least 1, avoiding underflow in the log.</p>
</section>
</section>
<section id="implementation-tips" class="level2">
<h2 class="anchored" data-anchor-id="implementation-tips">Implementation Tips</h2>
<p><strong>Vectorization:</strong> Use vectorized operations in libraries like NumPy/PyTorch instead of explicit loops whenever possible. This is significantly faster.</p>
<p><strong>Bad (slow):</strong></p>
<div class="sourceCode" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>A <span class="op">=</span> np.random.rand(<span class="dv">1000</span>, <span class="dv">1000</span>)</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a>B <span class="op">=</span> np.random.rand(<span class="dv">1000</span>, <span class="dv">1000</span>)</span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a>C <span class="op">=</span> np.zeros((<span class="dv">1000</span>, <span class="dv">1000</span>))</span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">1000</span>):</span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> j <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">1000</span>):</span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a>        C[i,j] <span class="op">=</span> A[i,j] <span class="op">+</span> B[i,j]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p><strong>Good (fast):</strong></p>
<div class="sourceCode" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a>C <span class="op">=</span> A <span class="op">+</span> B  <span class="co"># Vectorized operation</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p><strong>Broadcasting:</strong> Understand how broadcasting works in your framework to avoid unnecessary memory allocation and make code cleaner.</p>
<p><strong>Batch operations:</strong> Process multiple data examples simultaneously when possible. This leverages parallel hardware (GPUs) for massive speedups.</p>
</section>
</section>
<section id="common-mistakes-and-pitfalls" class="level1">
<h1>Common Mistakes and Pitfalls</h1>
<p>Learning matrix calculus involves avoiding several common traps.</p>
<p>Here are the most frequent mistakes and how to avoid them.</p>
<section id="dimension-mismatches" class="level2">
<h2 class="anchored" data-anchor-id="dimension-mismatches">Dimension Mismatches</h2>
<p><strong>The problem:</strong> Matrix operations are only defined when dimensions are compatible.</p>
<p><strong>Common mistake 1: Forgetting to transpose</strong></p>
<p><strong>Wrong:</strong> If <span class="math inline">\(\mathbf{A} \in \mathbb{R}^{m \times n}\)</span> and <span class="math inline">\(\mathbf{x} \in \mathbb{R}^{m}\)</span>, then <span class="math inline">\(\mathbf{A}\mathbf{x}\)</span> is undefined.</p>
<p><strong>Right:</strong> You probably meant <span class="math inline">\(\mathbf{A}^T\mathbf{x}\)</span> (if you want the result in <span class="math inline">\(\mathbb{R}^{n}\)</span>) or you have a mismatch in your variable definitions.</p>
<p><strong>How to avoid dimension mistakes:</strong></p>
<p><strong>Always write dimensions explicitly:</strong> Instead of writing <span class="math inline">\(\mathbf{A}\mathbf{x}\)</span>, write <span class="math inline">\(\mathbf{A}_{m \times n}\mathbf{x}_{n \times 1} = \mathbf{y}_{m \times 1}\)</span> during your derivation.</p>
</section>
<section id="chain-rule-errors" class="level2">
<h2 class="anchored" data-anchor-id="chain-rule-errors">Chain Rule Errors</h2>
<p><strong>The problem:</strong> Matrix multiplication is not commutative, so order matters in the chain rule.</p>
<p><strong>Wrong:</strong> If <span class="math inline">\(\mathbf{y} = f(\mathbf{u})\)</span> and <span class="math inline">\(\mathbf{u} = g(\mathbf{x})\)</span>, writing <span class="math inline">\(\frac{\partial \mathbf{y}}{\partial \mathbf{x}} = \frac{\partial \mathbf{u}}{\partial \mathbf{x}} \frac{\partial \mathbf{y}}{\partial \mathbf{u}}\)</span>.</p>
<p><strong>Right:</strong> <span class="math inline">\(\frac{\partial \mathbf{y}}{\partial \mathbf{x}} = \frac{\partial \mathbf{y}}{\partial \mathbf{u}} \frac{\partial \mathbf{u}}{\partial \mathbf{x}}\)</span>.</p>
<p><strong>How to avoid chain rule errors:</strong></p>
<p><strong>Always check dimensions:</strong> If <span class="math inline">\(\mathbf{y} \in \mathbb{R}^m, \mathbf{u} \in \mathbb{R}^n, \mathbf{x} \in \mathbb{R}^p\)</span>:</p>
<ul>
<li><span class="math inline">\(\frac{\partial \mathbf{y}}{\partial \mathbf{u}}\)</span> is <span class="math inline">\(m \times n\)</span></li>
<li><span class="math inline">\(\frac{\partial \mathbf{u}}{\partial \mathbf{x}}\)</span> is <span class="math inline">\(n \times p\)</span></li>
<li><span class="math inline">\(\frac{\partial \mathbf{y}}{\partial \mathbf{x}}\)</span> must be <span class="math inline">\(m \times p\)</span></li>
<li>Check: <span class="math inline">\((m \times n)(n \times p) \to (m \times p)\)</span> ✓</li>
</ul>
</section>
<section id="forgetting-symmetry" class="level2">
<h2 class="anchored" data-anchor-id="forgetting-symmetry">Forgetting Symmetry</h2>
<p><strong>The problem:</strong> Many important matrices (like Hessians and covariance matrices) are symmetric, but it’s easy to forget this property and use a more complex general formula.</p>
<p><strong>Missed optimization:</strong> If <span class="math inline">\(\mathbf{A}\)</span> is symmetric, then <span class="math inline">\(\frac{\partial}{\partial \mathbf{x}}(\mathbf{x}^T \mathbf{A} \mathbf{x}) = 2\mathbf{A}\mathbf{x}\)</span>, which is simpler than the general form <span class="math inline">\((\mathbf{A} + \mathbf{A}^T)\mathbf{x}\)</span>.</p>
<p><strong>How to avoid:</strong> Always check for symmetry before computing derivatives.</p>
</section>
</section>
<section id="practice-problems" class="level1">
<h1>Practice Problems</h1>
<p>Test your understanding with these carefully designed problems.</p>
<section id="basic-problems" class="level2">
<h2 class="anchored" data-anchor-id="basic-problems">Basic Problems</h2>
<p><strong>Problem 1: Simple Gradients</strong> Find the gradient of <span class="math inline">\(f(\mathbf{x}) = 3x_1^2 + 2x_1x_2 + x_2^2\)</span> where <span class="math inline">\(\mathbf{x} = [x_1, x_2]^T\)</span>.</p>
<div class="callout callout-style-default callout-tip callout-titled" title="Click for solution">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-22-contents" aria-controls="callout-22" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Click for solution
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-22" class="callout-22-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p><strong>Solution:</strong> <span class="math inline">\(\frac{\partial f}{\partial x_1} = \frac{\partial}{\partial x_1}(3x_1^2 + 2x_1x_2 + x_2^2) = 6x_1 + 2x_2\)</span> <span class="math inline">\(\frac{\partial f}{\partial x_2} = \frac{\partial}{\partial x_2}(3x_1^2 + 2x_1x_2 + x_2^2) = 2x_1 + 2x_2\)</span></p>
<p>Therefore: <span class="math inline">\(\nabla f = \begin{bmatrix} 6x_1 + 2x_2 \\ 2x_1 + 2x_2 \end{bmatrix}\)</span></p>
</div>
</div>
</div>
<p><strong>Problem 2: Simple Jacobian</strong> Find the Jacobian of <span class="math inline">\(\mathbf{f}(\mathbf{x}) = \begin{bmatrix} x_1 + x_2 \\ x_1 - x_2 \end{bmatrix}\)</span> where <span class="math inline">\(\mathbf{x} = [x_1, x_2]^T\)</span>.</p>
<div class="callout callout-style-default callout-tip callout-titled" title="Click for solution">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-23-contents" aria-controls="callout-23" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Click for solution
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-23" class="callout-23-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p><strong>Solution:</strong> The function <span class="math inline">\(\mathbf{f}\)</span> has 2 outputs and 2 inputs, so the Jacobian is <span class="math inline">\(2 \times 2\)</span>. <span class="math inline">\(f_1(\mathbf{x}) = x_1 + x_2\)</span> <span class="math inline">\(f_2(\mathbf{x}) = x_1 - x_2\)</span></p>
<p><span class="math display">\[\mathbf{J} = \begin{bmatrix} \frac{\partial f_1}{\partial x_1} &amp; \frac{\partial f_1}{\partial x_2} \\ \frac{\partial f_2}{\partial x_1} &amp; \frac{\partial f_2}{\partial x_2} \end{bmatrix} = \begin{bmatrix} 1 &amp; 1 \\ 1 &amp; -1 \end{bmatrix}\]</span></p>
</div>
</div>
</div>
</section>
<section id="intermediate-problems" class="level2">
<h2 class="anchored" data-anchor-id="intermediate-problems">Intermediate Problems</h2>
<p><strong>Problem 3: Quadratic Forms</strong> Find <span class="math inline">\(\frac{\partial}{\partial \mathbf{x}}(\mathbf{x}^T \mathbf{A} \mathbf{x})\)</span> where <span class="math inline">\(\mathbf{A} = \begin{bmatrix} 2 &amp; 1 \\ 1 &amp; 3 \end{bmatrix}\)</span>.</p>
<div class="callout callout-style-default callout-tip callout-titled" title="Click for solution">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-24-contents" aria-controls="callout-24" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Click for solution
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-24" class="callout-24-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p><strong>Solution:</strong> Since <span class="math inline">\(\mathbf{A}\)</span> is symmetric, we can use the simplified rule <span class="math inline">\(\frac{\partial}{\partial \mathbf{x}}(\mathbf{x}^T \mathbf{A} \mathbf{x}) = 2\mathbf{A}\mathbf{x}\)</span>.</p>
<p>Therefore: <span class="math inline">\(\frac{\partial}{\partial \mathbf{x}}(\mathbf{x}^T \mathbf{A} \mathbf{x}) = 2 \begin{bmatrix} 2 &amp; 1 \\ 1 &amp; 3 \end{bmatrix} \begin{bmatrix} x_1 \\ x_2 \end{bmatrix} = \begin{bmatrix} 4x_1 + 2x_2 \\ 2x_1 + 6x_2 \end{bmatrix}\)</span></p>
</div>
</div>
</div>
</section>
<section id="advanced-problems" class="level2">
<h2 class="anchored" data-anchor-id="advanced-problems">Advanced Problems</h2>
<p><strong>Problem 4: Optimization Application</strong> Use matrix calculus to find the value of <span class="math inline">\(\mathbf{x}\)</span> that minimizes <span class="math inline">\(f(\mathbf{x}) = \frac{1}{2}\mathbf{x}^T \mathbf{Q} \mathbf{x} - \mathbf{c}^T \mathbf{x}\)</span>, where <span class="math inline">\(\mathbf{Q}\)</span> is a symmetric positive definite matrix.</p>
<div class="callout callout-style-default callout-tip callout-titled" title="Click for solution">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-25-contents" aria-controls="callout-25" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Click for solution
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-25" class="callout-25-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p><strong>Solution:</strong> To find the minimum, we must find where the gradient is zero.</p>
<p><strong>Step 1:</strong> Find the gradient <span class="math inline">\(\nabla f = \frac{\partial}{\partial \mathbf{x}}(\frac{1}{2}\mathbf{x}^T \mathbf{Q} \mathbf{x}) - \frac{\partial}{\partial \mathbf{x}}(\mathbf{c}^T \mathbf{x})\)</span> Using our formulas for quadratic and linear forms (and since <span class="math inline">\(\mathbf{Q}\)</span> is symmetric): <span class="math inline">\(\nabla f = \frac{1}{2}(2\mathbf{Q}\mathbf{x}) - \mathbf{c} = \mathbf{Q}\mathbf{x} - \mathbf{c}\)</span></p>
<p><strong>Step 2:</strong> Set gradient to zero <span class="math inline">\(\nabla f = \mathbf{0}\)</span> <span class="math inline">\(\mathbf{Q}\mathbf{x} - \mathbf{c} = \mathbf{0}\)</span> <span class="math inline">\(\mathbf{Q}\mathbf{x} = \mathbf{c}\)</span> <span class="math inline">\(\mathbf{x}^* = \mathbf{Q}^{-1}\mathbf{c}\)</span></p>
<p><strong>Step 3:</strong> Verify it’s a minimum The Hessian is <span class="math inline">\(\mathbf{H} = \frac{\partial}{\partial \mathbf{x}^T}(\mathbf{Q}\mathbf{x} - \mathbf{c}) = \mathbf{Q}\)</span>. Since <span class="math inline">\(\mathbf{Q}\)</span> is positive definite, all its eigenvalues are positive, confirming that <span class="math inline">\(\mathbf{x}^*\)</span> is a local (and in this case, global) minimum.</p>
</div>
</div>
</div>
</section>
</section>
<section id="summary-and-key-takeaways" class="level1">
<h1>Summary and Key Takeaways</h1>
<section id="the-fundamental-structure" class="level2">
<h2 class="anchored" data-anchor-id="the-fundamental-structure">The Fundamental Structure</h2>
<p>The derivative’s structure depends on the function’s input and output:</p>
<ol type="1">
<li><strong>Scalar <span class="math inline">\(\to\)</span> Scalar:</strong> Ordinary derivative (a scalar)</li>
<li><strong>Vector <span class="math inline">\(\to\)</span> Scalar:</strong> Gradient (a vector)</li>
<li><strong>Matrix <span class="math inline">\(\to\)</span> Scalar:</strong> Matrix of partials (a matrix)</li>
<li><strong>Vector <span class="math inline">\(\to\)</span> Vector:</strong> Jacobian (a matrix)</li>
</ol>
</section>
<section id="the-shape-rule-is-your-friend" class="level2">
<h2 class="anchored" data-anchor-id="the-shape-rule-is-your-friend">The Shape Rule is Your Friend</h2>
<p>The dimensions of derivatives follow predictable patterns. Use this to check your work.</p>
<ul>
<li>Gradient: <span class="math inline">\(f: \mathbb{R}^n \to \mathbb{R}\)</span> gives derivative <span class="math inline">\(\in \mathbb{R}^n\)</span></li>
<li>Jacobian: <span class="math inline">\(\mathbf{f}: \mathbb{R}^n \to \mathbb{R}^m\)</span> gives derivative <span class="math inline">\(\in \mathbb{R}^{m \times n}\)</span></li>
</ul>
</section>
<section id="essential-formulas-to-remember" class="level2">
<h2 class="anchored" data-anchor-id="essential-formulas-to-remember">Essential Formulas to Remember</h2>
<ul>
<li><strong>Linear forms:</strong> <span class="math inline">\(\frac{\partial}{\partial \mathbf{x}}(\mathbf{a}^T\mathbf{x}) = \mathbf{a}\)</span></li>
<li><strong>Quadratic forms (A symmetric):</strong> <span class="math inline">\(\frac{\partial}{\partial \mathbf{x}}(\mathbf{x}^T\mathbf{A}\mathbf{x}) = 2\mathbf{A}\mathbf{x}\)</span></li>
<li><strong>Matrix Trace:</strong> <span class="math inline">\(\frac{\partial}{\partial \mathbf{X}} \text{tr}(\mathbf{A}\mathbf{X}) = \mathbf{A}^T\)</span></li>
</ul>
</section>
<section id="the-big-picture" class="level2">
<h2 class="anchored" data-anchor-id="the-big-picture">The Big Picture</h2>
<div class="callout callout-style-default callout-note callout-titled" title="Final Key Point">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Final Key Point
</div>
</div>
<div class="callout-body-container callout-body">
<p>Matrix calculus is not just a mathematical abstraction - it’s the computational foundation that makes modern machine learning possible. Every time you train a neural network, optimize a complex function, or analyze multivariate data, you’re using these principles. The most important skill is to think systematically about how small changes in inputs propagate through functions to affect the final outputs.</p>
</div>
</div>
</section>
</section>
<section id="quick-reference-guide" class="level1">
<h1>Quick Reference Guide</h1>
<section id="common-derivatives" class="level2">
<h2 class="anchored" data-anchor-id="common-derivatives">Common Derivatives</h2>
<table class="caption-top table">
<colgroup>
<col style="width: 24%">
<col style="width: 45%">
<col style="width: 30%">
</colgroup>
<thead>
<tr class="header">
<th>Expression</th>
<th>Derivative</th>
<th>Notes</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><span class="math inline">\(\mathbf{a}^T\mathbf{x}\)</span></td>
<td><span class="math inline">\(\mathbf{a}\)</span></td>
<td>Linear form</td>
</tr>
<tr class="even">
<td><span class="math inline">\(\mathbf{x}^T\mathbf{A}\mathbf{x}\)</span></td>
<td><span class="math inline">\((\mathbf{A} + \mathbf{A}^T)\mathbf{x}\)</span></td>
<td>Quadratic form (general)</td>
</tr>
<tr class="odd">
<td><span class="math inline">\(\mathbf{x}^T\mathbf{A}\mathbf{x}\)</span></td>
<td><span class="math inline">\(2\mathbf{A}\mathbf{x}\)</span></td>
<td>Quadratic form (A is symmetric)</td>
</tr>
<tr class="even">
<td><span class="math inline">\(\mathbf{x}^T\mathbf{x}\)</span></td>
<td><span class="math inline">\(2\mathbf{x}\)</span></td>
<td>Special case: <span class="math inline">\(\mathbf{A} = \mathbf{I}\)</span></td>
</tr>
<tr class="odd">
<td><span class="math inline">\(\text{tr}(\mathbf{A}\mathbf{X})\)</span></td>
<td><span class="math inline">\(\mathbf{A}^T\)</span></td>
<td>Trace of product</td>
</tr>
<tr class="even">
<td><span class="math inline">\(\det(\mathbf{X})\)</span></td>
<td><span class="math inline">\(\det(\mathbf{X})(\mathbf{X}^{-1})^T\)</span></td>
<td>Determinant</td>
</tr>
<tr class="odd">
<td><span class="math inline">\(\log \det(\mathbf{X})\)</span></td>
<td><span class="math inline">\((\mathbf{X}^{-1})^T\)</span></td>
<td>Log determinant</td>
</tr>
<tr class="even">
<td><span class="math inline">\(\mathbf{A}\mathbf{x}\)</span></td>
<td><span class="math inline">\(\mathbf{A}\)</span> (Jacobian, denominator layout) or <span class="math inline">\(\mathbf{A}^T\)</span> (numerator layout)</td>
<td>Linear transformation</td>
</tr>
</tbody>
</table>
</section>
<section id="shape-rules-quick-reference" class="level2">
<h2 class="anchored" data-anchor-id="shape-rules-quick-reference">Shape Rules Quick Reference</h2>
<table class="caption-top table">
<colgroup>
<col style="width: 16%">
<col style="width: 18%">
<col style="width: 18%">
<col style="width: 46%">
</colgroup>
<thead>
<tr class="header">
<th>Function Type</th>
<th>Variable Type</th>
<th>Derivative Shape</th>
<th>Example</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Scalar</td>
<td>Vector <span class="math inline">\(\in \mathbb{R}^n\)</span></td>
<td>Vector <span class="math inline">\(\in \mathbb{R}^n\)</span></td>
<td><span class="math inline">\(f: \mathbb{R}^n \to \mathbb{R}\)</span>, <span class="math inline">\(\frac{\partial f}{\partial \mathbf{x}}\)</span></td>
</tr>
<tr class="even">
<td>Scalar</td>
<td>Matrix <span class="math inline">\(\in \mathbb{R}^{m \times n}\)</span></td>
<td>Matrix <span class="math inline">\(\in \mathbb{R}^{m \times n}\)</span></td>
<td><span class="math inline">\(f: \mathbb{R}^{m \times n} \to \mathbb{R}\)</span>, <span class="math inline">\(\frac{\partial f}{\partial \mathbf{X}}\)</span></td>
</tr>
<tr class="odd">
<td>Vector <span class="math inline">\(\in \mathbb{R}^m\)</span></td>
<td>Vector <span class="math inline">\(\in \mathbb{R}^n\)</span></td>
<td>Matrix <span class="math inline">\(\in \mathbb{R}^{m \times n}\)</span></td>
<td><span class="math inline">\(\mathbf{f}: \mathbb{R}^n \to \mathbb{R}^m\)</span>, <span class="math inline">\(\frac{\partial \mathbf{f}}{\partial \mathbf{x}}\)</span></td>
</tr>
</tbody>
</table>
</section>
</section>

</main>
<!-- /main column -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
        const codeEl = trigger.previousElementSibling.cloneNode(true);
        for (const childEl of codeEl.children) {
          if (isCodeAnnotation(childEl)) {
            childEl.remove();
          }
        }
        return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
</div> <!-- /content -->




</body></html>